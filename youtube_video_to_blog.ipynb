{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.tools import tool\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import Image, display\n",
    "from typing_extensions import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from IPython.display import Markdown\n",
    "from langgraph.types import interrupt\n",
    "from langgraph.types import Command\n",
    "import os\n",
    "\n",
    "# Initialize memory saver\n",
    "memory = MemorySaver()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Initialize OpenAI and Groq chatbots\n",
    "os.environ[\"OPENAI_API_KEY\"]=os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "# Initialize Groq chatbot\n",
    "llm=ChatGroq(model=\"qwen-2.5-32b\")\n",
    "#llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "def get_youtube_transcript(video_url):\n",
    "    try:\n",
    "        # Extract Video ID from URL\n",
    "        video_id = video_url.split(\"v=\")[-1].split(\"&\")[0]\n",
    "\n",
    "        # Fetch transcript\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "\n",
    "        # Convert transcript list to text\n",
    "        transcript_text = \"\\n\".join([entry[\"text\"] for entry in transcript])\n",
    "\n",
    "        return transcript_text\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph state\n",
    "class State(TypedDict):\n",
    "    video_url: str\n",
    "    transcript: str\n",
    "    blog: str\n",
    "    review: str\n",
    "    human_feedback: str\n",
    "\n",
    "def transcript_generation(state: State) -> State:\n",
    "    # Fetch transcript\n",
    "    state[\"transcript\"] = get_youtube_transcript(state[\"video_url\"])\n",
    "    return state\n",
    "    \n",
    "def blog_generation(state: State) -> State:\n",
    "    if state.get(\"human_feedback\"):\n",
    "        state[\"blog\"] = llm.invoke(\n",
    "            f\"Generate a blog post for the transcript below:\\n{state['transcript']}\\n\"\n",
    "            f\"Consider the following feedback: {state['human_feedback']}\"\n",
    "        )\n",
    "    else:\n",
    "        state[\"blog\"] = llm.invoke(\n",
    "            f\"Generate a blog post for the transcript below:\\n{state['transcript']}\"\n",
    "        )\n",
    "    return state\n",
    "\n",
    "def review_generation(state: State) -> State:\n",
    "    # Generate review\n",
    "    state[\"review\"] = llm.invoke(f\"Generate review for the blog post below \\n {state['blog']}\")\n",
    "    return state\n",
    "\n",
    "def human_feedback1(state: State) -> State:\n",
    "    print(\"Awaiting human feedback...\")\n",
    "    feedback = interrupt(\"Please provide your feedback:\")\n",
    "    return {\"human_feedback\": feedback}\n",
    "    \n",
    "def should_continue(state: State) -> str:\n",
    "    if state.get(\"human_feedback\"):\n",
    "        return \"blogger\"\n",
    "    return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "builder = StateGraph(State)\n",
    "#Nodes\n",
    "builder.add_node(\"transcriptor\", transcript_generation)\n",
    "builder.add_node(\"blogger\", blog_generation)\n",
    "builder.add_node(\"reviewer\", review_generation)\n",
    "builder.add_node(\"human_analyst_feedback\", human_feedback1)\n",
    "\n",
    "#Edges\n",
    "builder.add_edge(START, \"transcriptor\")\n",
    "builder.add_edge(\"transcriptor\", \"blogger\")\n",
    "builder.add_edge(\"blogger\", \"reviewer\")\n",
    "builder.add_edge(\"reviewer\", \"human_analyst_feedback\")\n",
    "builder.add_conditional_edges(\"human_analyst_feedback\", should_continue, [\"blogger\", END])\n",
    "\n",
    "\n",
    "workflow = builder.compile(interrupt_before=['human_analyst_feedback'], checkpointer=memory)\n",
    "# Show the workflow\n",
    "display(Image(workflow.get_graph().draw_mermaid_png()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'video_url': 'https://www.youtube.com/watch?v=cy6EAp4iNN4'}\n",
      "{'video_url': 'https://www.youtube.com/watch?v=cy6EAp4iNN4', 'transcript': \"AI models are powerful tools but in\\norder to use them properly and securely\\nyou need to control them using an API so\\nin today's video I'm going to show you\\nhow to write a very simple python API to\\ncontrol access to an llm or an AI model\\nnow first I want to explain why you\\nactually need to do this you understand\\nthe security and the importance of\\nsetting this up now let's say that you\\nwant to use an llm something like chat\\nGPT or something like deep seek right if\\nthat's the case what you're probably\\ngoing to do is you're either going to\\nattempt to run this locally or you're\\ngoing to use the cloud provider so\\nyou're going to go to deepseeker open\\naai you're going to generate an API key\\nand then you're going to take that API\\nkey and that's what you send anytime you\\nwant to make a request or use the llm\\nnow this is great and it works well if\\nyou're doing this locally but in a\\nproduction environment if you were to\\ntake this API key and you were to use it\\ndirectly from your own front end so\\nsomething like a website or a mobile\\napplication you'd be introducing a huge\\nsecurity risk\\nthe reason why that's the case is\\nbecause if you use one of these keys\\nfrom your front end then anyone who has\\naccess to your front end code will be\\nable to see and use that key so they\\ncould really take advantage of that and\\nthey could send all kinds of requests\\nand cost you a ton of money because\\nanytime you send a request to something\\nlike a cloud provider will it cost you\\nmoney and even if you're running this\\nlocally on your own computer it costs\\nyou compute time or resources so the\\nmain idea here is that you only want to\\ninvoke an llm especially if it's it's\\ncoming from a provider like open aai\\nfrom something that's secure something\\nthat you control which would be a\\nbackend server or an API the basic flow\\nwould be if someone has access to your\\nfront-end application they would then\\nsend a request to your own backend from\\nyour backend you could then control\\nwhether or not you wanted to send a\\nrequest to the llm so you can control\\nroughly how much it will cost and which\\nof your users are allowed to use the llm\\nthis is common practice from all of the\\nAI applications that you've used before\\nand you'll typically notice that you get\\nsomething like credits so maybe you have\\n10 credits where you're able to call an\\nllm 10 times and then after that you\\nwould need to pay money to the service\\nprovider to compensate them for how much\\nthe llm is costing hopefully that makes\\na little bit of sense but the basic idea\\nhere is that we need to be able to\\ncontrol the calls to our llm so that we\\ncan decide which users can call it or\\nnot call it and make sure that it\\ndoesn't cost us too much money so with\\nthat in mind let's get into it and start\\nbuilding this out so like I mentioned\\nyou can control access to any llm that\\nyou want and you can use something like\\nopen AI or deep seek but in my case I'm\\njust going to run an llm locally on my\\nown computer using something called AMA\\nthis is completely free it's open source\\nand it lets you run models on your own\\nmachine assuming you have good enough\\nHardware I have an entire video on how\\nto set this up so I'll leave it on\\nscreen but I'll give you the cliff notes\\nHere what you need to do is simply\\ndownload ama if you want to set this up\\non your own computer so once you've\\ndownloaded and installed this you can\\nopen up a ter teral or a command prompt\\nand inside of here you can just type AMA\\nto make sure this is working if for some\\nreason the command isn't recognized you\\ncan try running olama by just typing the\\nname of the application if you're on\\nsomething like Windows and double\\nclicking it to run now once olama is\\nworking you can pull an olama model that\\nyou want to run locally now to do that\\nyou type a llama pull and then you put\\nthe name of the model in this case I'll\\nuse a model like mistro but you can use\\nmodels like llama 3 and any open source\\nmodel really you want you can pull it\\nand run it locally assuming you have\\nsufficient hardware and all of the\\nhardware requirements are specified on\\nthe olama website where it lists all of\\nthe different open- Source models so I'm\\ngoing to pull the mistal model I already\\nhave this downloaded so now it's on my\\nmachine and then if I wanted to use this\\nmodel I could type llama run and then\\nmistl and then I can just start chatting\\nwith the model like I would in any other\\ncase now to leave I can type slash bu\\nand that's great and now we'll be able\\nto use AMA from our python code again I\\nhave an entire video in case you're\\nconfused or more help that I'll leave on\\nscreen okay so now that we've done that\\nwhat we're going to do is start setting\\nup a basic API to use our llm model now\\nagain you can use any llm that you want\\nhere the important thing is that we just\\nset up the API and we secure it properly\\nso for this video the IDE that I'm going\\nto be using is pycharm now this is one\\nof the best idees when it comes to\\nworking with python especially for\\nthings like apis and dealing with\\nFrameworks like Fast API Jango or flask\\nwhich we'll be using in this video now I\\nhave a long-term partnership with py and\\nif you guys want to get access to the\\nprofessional Edition with an extended\\nfree trial of up to 3 months you can do\\nthat by clicking the link in the\\ndescription py charm has two versions\\nThe Community Edition which is\\ncompletely free and the professional\\nEdition which you do need to pay for but\\nagain because you're a viewer of this\\nchannel I can give it to you 3 months\\nfor free so you can try it out and see\\nif you like it it has all kinds of great\\nfeatures and we can actually test our\\nAPI directly from the IDE which makes\\nour life a lot easier and you'll see\\nthat in this video again links in the\\ndescription so first things first we're\\njust going to set up the dependencies\\nfor our python project to build this API\\nso I'm going to make a new file I just\\nopened a folder here in this IDE so I\\nwent up here and I just opened a folder\\nI called it API for llm and then from\\nhere I'm going to go new file and then\\nI'm going to call this\\nrequirements.txt\\nokay from this requirements.txt file I'm\\njust going to specify the dependencies\\nthat I'll need for my python project and\\nobviously you need python installed in\\norder to follow along with the rest of\\nthese steps so I'm going to install fast\\nAPI UV corn olama Python\\nd.v and then requests okay now fast API\\nis what we'll use for building our API\\nit'll be very simple don't worry uicorn\\nis for running our fast API application\\nolama is for interfacing with olama to\\nuse a local llm and then python. EnV is\\nfor loading in an environment variable\\nfile and request is for Center request\\nwhen we later test our API now we've\\nspecified these in our requirements.txt\\nso the next step is to Simply install\\nthem so what we're going to do is just\\nopen up a new terminal instance I just\\nhave one here and make sure it's in the\\nsame folder where your requirements.txt\\nfile is if you're opening the terminal\\nfrom an IDE or an editor then it should\\njust already be in the correct directory\\nfrom here assuming we have python\\ninstalled we're going to type pip\\ninstall - R and then requirements.txt\\nnow this is just going to install all of\\nthe dependencies into our main python\\ninstallation if you want you can use a\\nvirtual environment but I'm not going to\\ncover that in the name of time if that\\ncommand doesn't work you can try pip 3\\ninstall - R requirements.txt and what\\nthis does is install all of the\\ndependencies from the requirements file\\nnow that we have all of that installed\\nwe can start writing some simple code\\nand get an API up and running so what\\nI'm going to do is make a new file and\\nit's going to be a python file and I'm\\njust going to call this main.py you can\\ncall it anything you want but I\\nrecommend naming it main just to stick\\nwith the um kind of conventions I'm\\nusing in this tutorial it'll be a little\\nbit easier to follow along with okay so\\nfrom here we're going to make a very\\nsimple API and to do that we're going to\\nsay from Fast API import and then fast\\nAPI and then we're going to import o\\nLama okay then we're going to make an\\napp so we're going to say app is equal\\nto fast API and we're going to Define an\\nendpoint an endpoint is just a URL on\\nthis server that we can access so for\\nnow we're going to say ATA dopost and\\nwe're going to do slash generate now you\\ncan call this anything that you want but\\nwhat this specifies is that you need to\\nsend a post request which is a type of\\nHTTP request to this URL and then the\\nfunction that we write which will be\\ncalled generate will run whenever we go\\nto this/ generate rout now inside of\\nhere we're going to take a prompt which\\nis a string and what this specifies is\\nthat you need to pass a query parameter\\nwhich looks like this prompt is equal to\\nand then whatever the prompt is whenever\\nyou want to call this rout so we know\\nwhat the prompt to the llm should be\\nfrom here we can generate a simple\\nresponse by saying response is equal to\\nol. chat from the chat we're going to\\nsay the model is equal to whatever model\\nwe pulled and we want to use in this\\ncase it's mistal then we're going to say\\nmessages is equal to we're going to\\nspecify a list and we're going to put a\\npython dictionary in the python\\ndictionary we need to specify the role\\nas user and then the content as The\\nPrompt okay so you can pass multiple\\nmessages here but if you just want to\\npass one then you just do it like this\\nand obviously you can modify the call to\\nthe llm but I'm just keeping it basic\\nfor right now after here we're going to\\nreturn the response and the way we'll do\\nthis is we'll return a python dictionary\\nthat has response and then the response\\nand this is going to be the message and\\nthe content just so that we strip out\\nthe information that we actually want so\\nthat's literally it we're saying okay we\\nwant to chat with AMA this works because\\nwe've downloaded on our machine we\\nspecify the model we want to use the\\nmessages that we want to pass you could\\nalso pass a system message if you wanted\\nto customize the prompt or something and\\nthen you can have a response we say\\nresponse message content okay now we\\nneed to run the API so to run the API we\\ncan open up our terminal and make sure\\nagain you're in the same directory where\\nyour python script is and you can type\\nuicorn and then main colon app-- reload\\nnow what these variables are right here\\nmain is the name of my file so main.py\\nand then app is the name of my fast API\\napplication so app so if you've changed\\neither of those you need to adjust these\\nright here and the D- reload will run\\nthis in development mode so it will\\nreload the server anytime any changes\\nare made so if I hit enter here you can\\nsee that now the API is running it tells\\nus what port it's running on so Port\\n8000 on Local Host and now we need to\\ntest it now there's various ways to test\\nthe API but if you're working inside of\\npy charm then you can open up this\\nendpoints tab you can find it by\\nclicking on these three dots and look\\nfor it here or you can see kind of a\\ncircle inside of a half circle from here\\nyou're going to specify the main. apppp\\npackage okay you're going to press the\\nslash generate so it can automatically\\nactually read um the endpoint and then\\nwhat you can do is see the kind of\\nsample request right here so you can see\\nit says post local hostport 8000 SL\\ngenerate and then you'll need to add\\nthis query parameter question mark\\nprompt equal to and then whatever you\\nwant to prompt the llm with once you do\\nthat you can hit submit request you can\\nwait for this to run it will take a\\nsecond and then you can see the response\\nright here so let me just make this a\\nlittle bit bigger so that you guys can\\nread this and you can see that we get\\nthe response from the llm it says hello\\nworld it's a classic example blah blah\\nblah blah blah and you can use it as you\\nsee fit now other than that the easiest\\nway to test this if you're not using\\nsomething like py charm is to download a\\ntool called postmen so I'm just going to\\nopen it up right here this is a free\\ntool that you can download on your\\ncomputer whether it's window Mac Linux\\never that allows you to test your apis\\nreally easily so if you don't have\\nfamiliarity testing them simply download\\nand install this tool and then open it\\nup and I'll show you how to use it so\\nassuming our API is still running in our\\nterminal we can open up the postman\\napplication from here we can press this\\nlittle plus button you might need to\\nmake an account or it might ask you for\\nsome stuff you don't need to but just\\nget into this workspace press on plus\\nand then what you're going to do is\\nchange get here to say post because\\nwe're sending a post request from here\\nyou're going to type in the URL so HTTP\\ncolon colon SL localhost SL generate\\nokay and we need to make sure this is\\nPort 8000 so sorry Local Host Port 8000\\nSL generate question mark prompt is\\nequal to whatever you want to prompt so\\nsomething like hello world and then what\\nwe can do is we can press on send\\nPostman will automatically send the\\nrequest for us again you just need to\\nmake sure you have a URL that looks like\\nthis when we do this you can then see\\nthat we get the response back from the\\nllm\\nokay so now we've written a basic llm\\nbut we want to make this secure we want\\nto add some kind of authorization or at\\nleast some logic so that not anyone can\\njust use this so what I'm going to do\\nnow is adjust this code so that only\\nsomeone that has access to our function\\nor to our API will be able to use the\\nllm because after all that's the entire\\npoint of this we don't want to let just\\na random person use the llm we want to\\nmaybe control their access okay so now\\nlet's add our own version of an API key\\nnow this is a key that someone will need\\nto send to our backend server if they\\nwant to utilize the API now what we can\\ndo with this key is we can just give it\\nto certain users or we can generate it\\nhowever we want in any way that we see\\nfit and we can have some kind of value\\nattached to this key to limit the number\\nof requests that someone could send so\\nfor example any API key that I give to\\nsomeone maybe it only has a limit of 10\\nso after you've used the key 10 times\\nmaybe you need to pay more money or you\\nneed to wait a day or something\\nsomething like that before you could use\\nthe key again I'm going to set up some\\nbasic logic but it will give you a sense\\nof how to do this so what I'm going to\\ndo is I'm going to import a few things\\nso from Fast API I'm going to import\\ndepends HTTP exception and header okay\\nI'm then going to import OS and I'm\\ngoing to say from.\\nEnV import load. EnV now I'm going to\\ncall the load. EnV function and what\\nthis is going to do is load a value from\\nour environment variable iable file\\nwhich I'm going to create right now so\\ninside of our directory we're going to\\nmake a new file and we're going to call\\nthis EnV okay inside of here we're going\\nto say API key is equal to and then we\\ncan set this to some secret key that we\\nwould only give people that we want to\\nhave access to our API in order to use\\nthe llm now you could have as many API\\nKeys as you want and typically you would\\ngenerate these for users so when people\\nhave an account they would sign in and\\nthen you could issue them an API key and\\nthey need to use that API key to access\\nthe API that's a more advanced topic and\\nI'll leave a video on screen that shows\\nexactly how to do that but you'll get\\nthe idea with what I show you right here\\nso in this EnV file we've specified a\\nvariable called secret key now what\\nwe're going to do is we're going to load\\nin that value so we're going to say API\\nKeys is equal to a list or sorry is\\nequal to a set and inside of here we're\\ngoing to say os. getv and then the API\\nkey now this works because we've l\\nloaded in the environment variable file\\nand now we can get that environment\\nvariable and we put that in our list of\\nvalid API keys and if we wanted to we\\ncould associate this with a value like\\n10 and then we could subtract from this\\nvalue anytime someone uses the API and\\ntreat this as like their credits so I\\ncan say you know this key has five\\ncredits remaining so we could change\\nthis to API key credits or something and\\nyou know what let's go ahead and do that\\nokay now that we've done that we're\\ngoing to write a simple function that\\nwill verify that a user has an API key\\nbefore we allow them to continue so I'm\\ngoing to say Define verify uncore aior\\nkey and we're going to specify xcore\\naior key we're going to say this is a\\nstring and this is equal to header and\\nthen none okay now what this is going to\\ndo is it's going to look in the headers\\nof our request which I'll show you in\\none second for a variable called X API\\nkey if finds it it's going to associate\\nit with this parameter right here and\\nthen allow us to read it in and use it\\ninside of this um what do you call it\\nfunction so what we're actually going to\\ndo here is we're going to say credits is\\nequal to API key credits. getet xapi key\\nnow we're going to specify the default\\nvalue is zero and what this means is\\nthat we're going to attempt to get\\nwhatever the number of credits are\\nassociated with our API key but if that\\nAPI key doesn't exist then we're just\\ngoing to get zero then we're simply\\ngoing to check if the credits are equal\\nto zero so we're going to say if the\\ncredits are less than or equal to zero\\nthen what we're going to do is we're\\ngoing to return or sorry we're going to\\nraise an HTTP exception and we're going\\nto say status 401 and then we're going\\nto say invalid API key or no credits\\nokay just telling them hey you don't\\nhave any credits left or your API key is\\nexpired or actually from here we can\\njust return the X API key so if they do\\nhave a valid key we can return it so we\\ncan use it later on and then inside of\\nhere we're going to we're going to make\\nsure that they have an API key and that\\nit's valid before we allow them to use\\nthis function so to do that inside of\\nthe parameters we're going to add X _\\nAPI key we're going to say this is type\\nstring and this is equal to depends on\\nverify API key now just let me use this\\nvariable and then it will light up so\\nyou can read it easier but what this\\nmeans is that we're depending on this\\nfunction call and then if this function\\ncall raises an error we just won't do\\nanything inside of here whereas if it\\nReturns the API key then we're good to\\ngo and we'll enter this function so\\ninside of here the first thing we're\\ngoing to do is just subtract our credits\\nso we're going to say API key credits at\\nX API key and then we're just going to\\nsay minus equals 1 so we're just going\\nto subtract one from that to specify hey\\nwe're removing one of your credits\\nbecause now you've successfully used\\nthis function and that's it that's\\nliterally all that we need and now we've\\nadded kind of control to this API where\\nyou can only use this if you pass us an\\nAPI key now again we've just hardcoded\\nan API key here from our environment\\nvariable file F but you could\\ndynamically generate these however you\\nwanted to and you can handle them for\\nvarious users so I'm showing you the\\nlogic but obviously you would need to go\\na step further with this to have correct\\nauthentication and authorization for a\\nlarger Scale app so now that we've done\\nthis we want to test the authorized API\\nso we should be able to rerun this so\\nhere it looks like it was rerunning but\\nI'm just going to shut it down and\\nrestart it to make sure it loads in the\\nenvironment variable so same thing\\nuvicorn main app reload so now to test\\nthe API again there's multiple ways to\\ndo that but I'm just going to use\\nPostman so that it's universally\\navailable for all of you and I'm going\\nto keep the same request that I had last\\ntime except this time I'm going to go\\ninto headers now inside of headers here\\nI'm just going to add the header for my\\nAPI key so I'm going to add the header\\nof xcore API uncore key like that I\\nbelieve that's what we called it and\\nthen for the value I'm going to say\\nsecret key okay all right so a silly\\nmistake here but actually we want to use\\nuh High when we specify this in the\\nheader not underscores so just x-\\napi-key and then put the value secret\\nkey you don't need any quotes and now\\nwhen we send this you should see that it\\nworks it just takes a second and then we\\nget the response back and if we try to\\ndo this now five times let's just keep\\ndoing it so that our credits go lower\\nand lower you should see eventually when\\nwe get to the fifth request that it\\ndoesn't work and it tells us that we run\\nout of tokens and you can see that's\\nwhat we get it says invalid API key or\\nno credits so there you go we've now\\nbuilt an authenticated API that allows\\nus to utilize our llm with this API key\\nnow again you want to manage these Keys\\ncorrectly there's a lot of other logic\\nand things you can do and I will leave a\\nvideo on screen that will help you do\\nthat now beyond that I'm quickly just\\ngoing to show you a python script that\\nyou can use if you wanted to test the\\nAPI from python code so I'm just going\\nto make a script called test API now\\nwhat I'm going to do is just copy in\\nsome code so here is the code you can\\nsee that I import requests I import the\\ncode to load my environment variable\\nfile I specify the URL that I want to\\nsend my request to and the prompt that I\\nwant to pass to the llm I then specify\\nmy headers so I have my API key which is\\nequal to what I'm loading from the\\nenvironment variable file content type\\napplication Json send a post request\\nwith my URL and my headers and then I\\nprint out the response. Json now I just\\nrestarted my server I took a quick cut\\nso when I restarted the server it will\\nreset the amount of credits that I have\\nand now if I run this file just takes a\\nsecond and we should see that we get our\\nresponse and we do you can see that I\\nget the response back from the llm okay\\nso that's pretty much all I wanted to\\nshow you in this video I know there was\\na lot but I wanted to explain how you\\nset up a simple API with this API token\\nwhich you can then use to control access\\nto the llm the llm call here can be\\nanything that you want but it's\\nimportant that you do gate it and you do\\ncontrol it from a backend server so that\\nfront-end users can't directly access\\nyour tokens now you may be asking well\\nwhat about this token right here the\\nwhole point of this token is that you\\ncontrol this token rather than open AI\\nor deep seek or something like that so\\nnow if you want to invalidate the token\\nif you want to have a certain number of\\ncredits if you only want to give it to\\npremium users or paid users you can do\\nthat right you can control how you want\\nto issue this token but if you just use\\nan openai token now anyone that has\\naccess to that can go and utilize open\\naai and cost you a ton of money so\\nthat's why you want to set up this own\\nkind of custom back end where you have\\nfull control as the developer and the\\napp owner I will leave this code in the\\ndescription in case you want to check it\\nout it'll be available from GitHub if\\nyou enjoyed the video make sure to leave\\na like subscribe to the channel and I\\nwill see you in the next one\\n[Music]\"}\n",
      "{'video_url': 'https://www.youtube.com/watch?v=cy6EAp4iNN4', 'transcript': \"AI models are powerful tools but in\\norder to use them properly and securely\\nyou need to control them using an API so\\nin today's video I'm going to show you\\nhow to write a very simple python API to\\ncontrol access to an llm or an AI model\\nnow first I want to explain why you\\nactually need to do this you understand\\nthe security and the importance of\\nsetting this up now let's say that you\\nwant to use an llm something like chat\\nGPT or something like deep seek right if\\nthat's the case what you're probably\\ngoing to do is you're either going to\\nattempt to run this locally or you're\\ngoing to use the cloud provider so\\nyou're going to go to deepseeker open\\naai you're going to generate an API key\\nand then you're going to take that API\\nkey and that's what you send anytime you\\nwant to make a request or use the llm\\nnow this is great and it works well if\\nyou're doing this locally but in a\\nproduction environment if you were to\\ntake this API key and you were to use it\\ndirectly from your own front end so\\nsomething like a website or a mobile\\napplication you'd be introducing a huge\\nsecurity risk\\nthe reason why that's the case is\\nbecause if you use one of these keys\\nfrom your front end then anyone who has\\naccess to your front end code will be\\nable to see and use that key so they\\ncould really take advantage of that and\\nthey could send all kinds of requests\\nand cost you a ton of money because\\nanytime you send a request to something\\nlike a cloud provider will it cost you\\nmoney and even if you're running this\\nlocally on your own computer it costs\\nyou compute time or resources so the\\nmain idea here is that you only want to\\ninvoke an llm especially if it's it's\\ncoming from a provider like open aai\\nfrom something that's secure something\\nthat you control which would be a\\nbackend server or an API the basic flow\\nwould be if someone has access to your\\nfront-end application they would then\\nsend a request to your own backend from\\nyour backend you could then control\\nwhether or not you wanted to send a\\nrequest to the llm so you can control\\nroughly how much it will cost and which\\nof your users are allowed to use the llm\\nthis is common practice from all of the\\nAI applications that you've used before\\nand you'll typically notice that you get\\nsomething like credits so maybe you have\\n10 credits where you're able to call an\\nllm 10 times and then after that you\\nwould need to pay money to the service\\nprovider to compensate them for how much\\nthe llm is costing hopefully that makes\\na little bit of sense but the basic idea\\nhere is that we need to be able to\\ncontrol the calls to our llm so that we\\ncan decide which users can call it or\\nnot call it and make sure that it\\ndoesn't cost us too much money so with\\nthat in mind let's get into it and start\\nbuilding this out so like I mentioned\\nyou can control access to any llm that\\nyou want and you can use something like\\nopen AI or deep seek but in my case I'm\\njust going to run an llm locally on my\\nown computer using something called AMA\\nthis is completely free it's open source\\nand it lets you run models on your own\\nmachine assuming you have good enough\\nHardware I have an entire video on how\\nto set this up so I'll leave it on\\nscreen but I'll give you the cliff notes\\nHere what you need to do is simply\\ndownload ama if you want to set this up\\non your own computer so once you've\\ndownloaded and installed this you can\\nopen up a ter teral or a command prompt\\nand inside of here you can just type AMA\\nto make sure this is working if for some\\nreason the command isn't recognized you\\ncan try running olama by just typing the\\nname of the application if you're on\\nsomething like Windows and double\\nclicking it to run now once olama is\\nworking you can pull an olama model that\\nyou want to run locally now to do that\\nyou type a llama pull and then you put\\nthe name of the model in this case I'll\\nuse a model like mistro but you can use\\nmodels like llama 3 and any open source\\nmodel really you want you can pull it\\nand run it locally assuming you have\\nsufficient hardware and all of the\\nhardware requirements are specified on\\nthe olama website where it lists all of\\nthe different open- Source models so I'm\\ngoing to pull the mistal model I already\\nhave this downloaded so now it's on my\\nmachine and then if I wanted to use this\\nmodel I could type llama run and then\\nmistl and then I can just start chatting\\nwith the model like I would in any other\\ncase now to leave I can type slash bu\\nand that's great and now we'll be able\\nto use AMA from our python code again I\\nhave an entire video in case you're\\nconfused or more help that I'll leave on\\nscreen okay so now that we've done that\\nwhat we're going to do is start setting\\nup a basic API to use our llm model now\\nagain you can use any llm that you want\\nhere the important thing is that we just\\nset up the API and we secure it properly\\nso for this video the IDE that I'm going\\nto be using is pycharm now this is one\\nof the best idees when it comes to\\nworking with python especially for\\nthings like apis and dealing with\\nFrameworks like Fast API Jango or flask\\nwhich we'll be using in this video now I\\nhave a long-term partnership with py and\\nif you guys want to get access to the\\nprofessional Edition with an extended\\nfree trial of up to 3 months you can do\\nthat by clicking the link in the\\ndescription py charm has two versions\\nThe Community Edition which is\\ncompletely free and the professional\\nEdition which you do need to pay for but\\nagain because you're a viewer of this\\nchannel I can give it to you 3 months\\nfor free so you can try it out and see\\nif you like it it has all kinds of great\\nfeatures and we can actually test our\\nAPI directly from the IDE which makes\\nour life a lot easier and you'll see\\nthat in this video again links in the\\ndescription so first things first we're\\njust going to set up the dependencies\\nfor our python project to build this API\\nso I'm going to make a new file I just\\nopened a folder here in this IDE so I\\nwent up here and I just opened a folder\\nI called it API for llm and then from\\nhere I'm going to go new file and then\\nI'm going to call this\\nrequirements.txt\\nokay from this requirements.txt file I'm\\njust going to specify the dependencies\\nthat I'll need for my python project and\\nobviously you need python installed in\\norder to follow along with the rest of\\nthese steps so I'm going to install fast\\nAPI UV corn olama Python\\nd.v and then requests okay now fast API\\nis what we'll use for building our API\\nit'll be very simple don't worry uicorn\\nis for running our fast API application\\nolama is for interfacing with olama to\\nuse a local llm and then python. EnV is\\nfor loading in an environment variable\\nfile and request is for Center request\\nwhen we later test our API now we've\\nspecified these in our requirements.txt\\nso the next step is to Simply install\\nthem so what we're going to do is just\\nopen up a new terminal instance I just\\nhave one here and make sure it's in the\\nsame folder where your requirements.txt\\nfile is if you're opening the terminal\\nfrom an IDE or an editor then it should\\njust already be in the correct directory\\nfrom here assuming we have python\\ninstalled we're going to type pip\\ninstall - R and then requirements.txt\\nnow this is just going to install all of\\nthe dependencies into our main python\\ninstallation if you want you can use a\\nvirtual environment but I'm not going to\\ncover that in the name of time if that\\ncommand doesn't work you can try pip 3\\ninstall - R requirements.txt and what\\nthis does is install all of the\\ndependencies from the requirements file\\nnow that we have all of that installed\\nwe can start writing some simple code\\nand get an API up and running so what\\nI'm going to do is make a new file and\\nit's going to be a python file and I'm\\njust going to call this main.py you can\\ncall it anything you want but I\\nrecommend naming it main just to stick\\nwith the um kind of conventions I'm\\nusing in this tutorial it'll be a little\\nbit easier to follow along with okay so\\nfrom here we're going to make a very\\nsimple API and to do that we're going to\\nsay from Fast API import and then fast\\nAPI and then we're going to import o\\nLama okay then we're going to make an\\napp so we're going to say app is equal\\nto fast API and we're going to Define an\\nendpoint an endpoint is just a URL on\\nthis server that we can access so for\\nnow we're going to say ATA dopost and\\nwe're going to do slash generate now you\\ncan call this anything that you want but\\nwhat this specifies is that you need to\\nsend a post request which is a type of\\nHTTP request to this URL and then the\\nfunction that we write which will be\\ncalled generate will run whenever we go\\nto this/ generate rout now inside of\\nhere we're going to take a prompt which\\nis a string and what this specifies is\\nthat you need to pass a query parameter\\nwhich looks like this prompt is equal to\\nand then whatever the prompt is whenever\\nyou want to call this rout so we know\\nwhat the prompt to the llm should be\\nfrom here we can generate a simple\\nresponse by saying response is equal to\\nol. chat from the chat we're going to\\nsay the model is equal to whatever model\\nwe pulled and we want to use in this\\ncase it's mistal then we're going to say\\nmessages is equal to we're going to\\nspecify a list and we're going to put a\\npython dictionary in the python\\ndictionary we need to specify the role\\nas user and then the content as The\\nPrompt okay so you can pass multiple\\nmessages here but if you just want to\\npass one then you just do it like this\\nand obviously you can modify the call to\\nthe llm but I'm just keeping it basic\\nfor right now after here we're going to\\nreturn the response and the way we'll do\\nthis is we'll return a python dictionary\\nthat has response and then the response\\nand this is going to be the message and\\nthe content just so that we strip out\\nthe information that we actually want so\\nthat's literally it we're saying okay we\\nwant to chat with AMA this works because\\nwe've downloaded on our machine we\\nspecify the model we want to use the\\nmessages that we want to pass you could\\nalso pass a system message if you wanted\\nto customize the prompt or something and\\nthen you can have a response we say\\nresponse message content okay now we\\nneed to run the API so to run the API we\\ncan open up our terminal and make sure\\nagain you're in the same directory where\\nyour python script is and you can type\\nuicorn and then main colon app-- reload\\nnow what these variables are right here\\nmain is the name of my file so main.py\\nand then app is the name of my fast API\\napplication so app so if you've changed\\neither of those you need to adjust these\\nright here and the D- reload will run\\nthis in development mode so it will\\nreload the server anytime any changes\\nare made so if I hit enter here you can\\nsee that now the API is running it tells\\nus what port it's running on so Port\\n8000 on Local Host and now we need to\\ntest it now there's various ways to test\\nthe API but if you're working inside of\\npy charm then you can open up this\\nendpoints tab you can find it by\\nclicking on these three dots and look\\nfor it here or you can see kind of a\\ncircle inside of a half circle from here\\nyou're going to specify the main. apppp\\npackage okay you're going to press the\\nslash generate so it can automatically\\nactually read um the endpoint and then\\nwhat you can do is see the kind of\\nsample request right here so you can see\\nit says post local hostport 8000 SL\\ngenerate and then you'll need to add\\nthis query parameter question mark\\nprompt equal to and then whatever you\\nwant to prompt the llm with once you do\\nthat you can hit submit request you can\\nwait for this to run it will take a\\nsecond and then you can see the response\\nright here so let me just make this a\\nlittle bit bigger so that you guys can\\nread this and you can see that we get\\nthe response from the llm it says hello\\nworld it's a classic example blah blah\\nblah blah blah and you can use it as you\\nsee fit now other than that the easiest\\nway to test this if you're not using\\nsomething like py charm is to download a\\ntool called postmen so I'm just going to\\nopen it up right here this is a free\\ntool that you can download on your\\ncomputer whether it's window Mac Linux\\never that allows you to test your apis\\nreally easily so if you don't have\\nfamiliarity testing them simply download\\nand install this tool and then open it\\nup and I'll show you how to use it so\\nassuming our API is still running in our\\nterminal we can open up the postman\\napplication from here we can press this\\nlittle plus button you might need to\\nmake an account or it might ask you for\\nsome stuff you don't need to but just\\nget into this workspace press on plus\\nand then what you're going to do is\\nchange get here to say post because\\nwe're sending a post request from here\\nyou're going to type in the URL so HTTP\\ncolon colon SL localhost SL generate\\nokay and we need to make sure this is\\nPort 8000 so sorry Local Host Port 8000\\nSL generate question mark prompt is\\nequal to whatever you want to prompt so\\nsomething like hello world and then what\\nwe can do is we can press on send\\nPostman will automatically send the\\nrequest for us again you just need to\\nmake sure you have a URL that looks like\\nthis when we do this you can then see\\nthat we get the response back from the\\nllm\\nokay so now we've written a basic llm\\nbut we want to make this secure we want\\nto add some kind of authorization or at\\nleast some logic so that not anyone can\\njust use this so what I'm going to do\\nnow is adjust this code so that only\\nsomeone that has access to our function\\nor to our API will be able to use the\\nllm because after all that's the entire\\npoint of this we don't want to let just\\na random person use the llm we want to\\nmaybe control their access okay so now\\nlet's add our own version of an API key\\nnow this is a key that someone will need\\nto send to our backend server if they\\nwant to utilize the API now what we can\\ndo with this key is we can just give it\\nto certain users or we can generate it\\nhowever we want in any way that we see\\nfit and we can have some kind of value\\nattached to this key to limit the number\\nof requests that someone could send so\\nfor example any API key that I give to\\nsomeone maybe it only has a limit of 10\\nso after you've used the key 10 times\\nmaybe you need to pay more money or you\\nneed to wait a day or something\\nsomething like that before you could use\\nthe key again I'm going to set up some\\nbasic logic but it will give you a sense\\nof how to do this so what I'm going to\\ndo is I'm going to import a few things\\nso from Fast API I'm going to import\\ndepends HTTP exception and header okay\\nI'm then going to import OS and I'm\\ngoing to say from.\\nEnV import load. EnV now I'm going to\\ncall the load. EnV function and what\\nthis is going to do is load a value from\\nour environment variable iable file\\nwhich I'm going to create right now so\\ninside of our directory we're going to\\nmake a new file and we're going to call\\nthis EnV okay inside of here we're going\\nto say API key is equal to and then we\\ncan set this to some secret key that we\\nwould only give people that we want to\\nhave access to our API in order to use\\nthe llm now you could have as many API\\nKeys as you want and typically you would\\ngenerate these for users so when people\\nhave an account they would sign in and\\nthen you could issue them an API key and\\nthey need to use that API key to access\\nthe API that's a more advanced topic and\\nI'll leave a video on screen that shows\\nexactly how to do that but you'll get\\nthe idea with what I show you right here\\nso in this EnV file we've specified a\\nvariable called secret key now what\\nwe're going to do is we're going to load\\nin that value so we're going to say API\\nKeys is equal to a list or sorry is\\nequal to a set and inside of here we're\\ngoing to say os. getv and then the API\\nkey now this works because we've l\\nloaded in the environment variable file\\nand now we can get that environment\\nvariable and we put that in our list of\\nvalid API keys and if we wanted to we\\ncould associate this with a value like\\n10 and then we could subtract from this\\nvalue anytime someone uses the API and\\ntreat this as like their credits so I\\ncan say you know this key has five\\ncredits remaining so we could change\\nthis to API key credits or something and\\nyou know what let's go ahead and do that\\nokay now that we've done that we're\\ngoing to write a simple function that\\nwill verify that a user has an API key\\nbefore we allow them to continue so I'm\\ngoing to say Define verify uncore aior\\nkey and we're going to specify xcore\\naior key we're going to say this is a\\nstring and this is equal to header and\\nthen none okay now what this is going to\\ndo is it's going to look in the headers\\nof our request which I'll show you in\\none second for a variable called X API\\nkey if finds it it's going to associate\\nit with this parameter right here and\\nthen allow us to read it in and use it\\ninside of this um what do you call it\\nfunction so what we're actually going to\\ndo here is we're going to say credits is\\nequal to API key credits. getet xapi key\\nnow we're going to specify the default\\nvalue is zero and what this means is\\nthat we're going to attempt to get\\nwhatever the number of credits are\\nassociated with our API key but if that\\nAPI key doesn't exist then we're just\\ngoing to get zero then we're simply\\ngoing to check if the credits are equal\\nto zero so we're going to say if the\\ncredits are less than or equal to zero\\nthen what we're going to do is we're\\ngoing to return or sorry we're going to\\nraise an HTTP exception and we're going\\nto say status 401 and then we're going\\nto say invalid API key or no credits\\nokay just telling them hey you don't\\nhave any credits left or your API key is\\nexpired or actually from here we can\\njust return the X API key so if they do\\nhave a valid key we can return it so we\\ncan use it later on and then inside of\\nhere we're going to we're going to make\\nsure that they have an API key and that\\nit's valid before we allow them to use\\nthis function so to do that inside of\\nthe parameters we're going to add X _\\nAPI key we're going to say this is type\\nstring and this is equal to depends on\\nverify API key now just let me use this\\nvariable and then it will light up so\\nyou can read it easier but what this\\nmeans is that we're depending on this\\nfunction call and then if this function\\ncall raises an error we just won't do\\nanything inside of here whereas if it\\nReturns the API key then we're good to\\ngo and we'll enter this function so\\ninside of here the first thing we're\\ngoing to do is just subtract our credits\\nso we're going to say API key credits at\\nX API key and then we're just going to\\nsay minus equals 1 so we're just going\\nto subtract one from that to specify hey\\nwe're removing one of your credits\\nbecause now you've successfully used\\nthis function and that's it that's\\nliterally all that we need and now we've\\nadded kind of control to this API where\\nyou can only use this if you pass us an\\nAPI key now again we've just hardcoded\\nan API key here from our environment\\nvariable file F but you could\\ndynamically generate these however you\\nwanted to and you can handle them for\\nvarious users so I'm showing you the\\nlogic but obviously you would need to go\\na step further with this to have correct\\nauthentication and authorization for a\\nlarger Scale app so now that we've done\\nthis we want to test the authorized API\\nso we should be able to rerun this so\\nhere it looks like it was rerunning but\\nI'm just going to shut it down and\\nrestart it to make sure it loads in the\\nenvironment variable so same thing\\nuvicorn main app reload so now to test\\nthe API again there's multiple ways to\\ndo that but I'm just going to use\\nPostman so that it's universally\\navailable for all of you and I'm going\\nto keep the same request that I had last\\ntime except this time I'm going to go\\ninto headers now inside of headers here\\nI'm just going to add the header for my\\nAPI key so I'm going to add the header\\nof xcore API uncore key like that I\\nbelieve that's what we called it and\\nthen for the value I'm going to say\\nsecret key okay all right so a silly\\nmistake here but actually we want to use\\nuh High when we specify this in the\\nheader not underscores so just x-\\napi-key and then put the value secret\\nkey you don't need any quotes and now\\nwhen we send this you should see that it\\nworks it just takes a second and then we\\nget the response back and if we try to\\ndo this now five times let's just keep\\ndoing it so that our credits go lower\\nand lower you should see eventually when\\nwe get to the fifth request that it\\ndoesn't work and it tells us that we run\\nout of tokens and you can see that's\\nwhat we get it says invalid API key or\\nno credits so there you go we've now\\nbuilt an authenticated API that allows\\nus to utilize our llm with this API key\\nnow again you want to manage these Keys\\ncorrectly there's a lot of other logic\\nand things you can do and I will leave a\\nvideo on screen that will help you do\\nthat now beyond that I'm quickly just\\ngoing to show you a python script that\\nyou can use if you wanted to test the\\nAPI from python code so I'm just going\\nto make a script called test API now\\nwhat I'm going to do is just copy in\\nsome code so here is the code you can\\nsee that I import requests I import the\\ncode to load my environment variable\\nfile I specify the URL that I want to\\nsend my request to and the prompt that I\\nwant to pass to the llm I then specify\\nmy headers so I have my API key which is\\nequal to what I'm loading from the\\nenvironment variable file content type\\napplication Json send a post request\\nwith my URL and my headers and then I\\nprint out the response. Json now I just\\nrestarted my server I took a quick cut\\nso when I restarted the server it will\\nreset the amount of credits that I have\\nand now if I run this file just takes a\\nsecond and we should see that we get our\\nresponse and we do you can see that I\\nget the response back from the llm okay\\nso that's pretty much all I wanted to\\nshow you in this video I know there was\\na lot but I wanted to explain how you\\nset up a simple API with this API token\\nwhich you can then use to control access\\nto the llm the llm call here can be\\nanything that you want but it's\\nimportant that you do gate it and you do\\ncontrol it from a backend server so that\\nfront-end users can't directly access\\nyour tokens now you may be asking well\\nwhat about this token right here the\\nwhole point of this token is that you\\ncontrol this token rather than open AI\\nor deep seek or something like that so\\nnow if you want to invalidate the token\\nif you want to have a certain number of\\ncredits if you only want to give it to\\npremium users or paid users you can do\\nthat right you can control how you want\\nto issue this token but if you just use\\nan openai token now anyone that has\\naccess to that can go and utilize open\\naai and cost you a ton of money so\\nthat's why you want to set up this own\\nkind of custom back end where you have\\nfull control as the developer and the\\napp owner I will leave this code in the\\ndescription in case you want to check it\\nout it'll be available from GitHub if\\nyou enjoyed the video make sure to leave\\na like subscribe to the channel and I\\nwill see you in the next one\\n[Music]\", 'blog': AIMessage(content='### Controlling Access to AI Models with a Secure Python API\\n\\nAI models have revolutionized the way we interact with data and automate tasks, but with great power comes great responsibility. Ensuring these models are used securely and responsibly is crucial, especially when it comes to managing costs and user access. In this blog post, I’ll guide you through creating a secure Python API to control access to an AI model, using a local language model (LLM) via a tool called AMA. This method allows you to manage who can access your AI model, ensuring your resources aren’t abused.\\n\\n#### Why Control Access?\\n\\nAccess control is paramount when integrating AI models into production environments. For instance, using a model like OpenAI’s GPT or DeepSeek’s DeepSeeker directly from a front-end application introduces significant security risks. If you embed an API key directly into your front-end code, anyone can potentially access and misuse it, leading to unauthorized usage and significant financial costs.\\n\\nTo mitigate this, you should control the model\\'s access from a backend server, where you can manage requests, user permissions, and resource costs.\\n\\n#### Setting Up the Environment\\n\\nBefore diving into the code, you need to have your environment ready to run a local LLM. Using AMA, a tool for running language models locally, you can achieve this without the need for cloud-based resources. You’ll need to install AMA and pull a model to run locally.\\n\\n1. **Install AMA**: Follow the instructions on the AMA GitHub or documentation to install it.\\n2. **Pull a Model**: Use the command `ama pull mistral` to download and set up a model like Mistral.\\n\\n#### Building the API\\n\\nTo build the API, you\\'ll use FastAPI, a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\\n\\n1. **Setup Dependencies**: Create a `requirements.txt` file with the dependencies (`fastapi`, `uvicorn`, `olama`, `python-dotenv`, `requests`).\\n2. **Install Dependencies**: Use `pip install -r requirements.txt` to install the necessary libraries.\\n3. **Create the API Script**: Create a `main.py` file where the magic happens.\\n\\n```python\\nfrom fastapi import FastAPI, Depends, HTTPException\\nfrom olama import chat\\nimport os\\nfrom dotenv import load_dotenv\\nimport requests\\n\\napp = FastAPI()\\n\\nload_dotenv()\\nAPI_KEY = os.getenv(\\'API_KEY\\')\\nAPI_KEY_CREDITS = {API_KEY: 5}  # Setting up credits for the API key\\n\\ndef verify_api_key(x_api_key: str = Depends()):\\n    credits = API_KEY_CREDITS.get(x_api_key, 0)\\n    if credits <= 0:\\n        raise HTTPException(status_code=401, detail=\"Invalid API key or no credits\")\\n    API_KEY_CREDITS[x_api_key] -= 1\\n    return x_api_key\\n\\n@app.post(\"/generate\")\\ndef generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\\n    response = chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\\n    return {\"response\": response}\\n\\nif __name__ == \"__main__\":\\n    import uvicorn\\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\\n```\\n\\n#### Testing the API\\n\\nTo test the API, you can use a tool like Postman or write a simple Python script.\\n\\n1. **Using Postman**: Set up a POST request to `http://localhost:8000/generate` with the header `X-API-KEY` and the value of your secret key.\\n\\n2. **Python Script**: Here’s a simple script to test your API:\\n\\n```python\\nimport requests\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nAPI_URL = \"http://localhost:8000/generate\"\\nAPI_KEY = os.getenv(\\'API_KEY\\')\\nPROMPT = \"What is the meaning of life?\"\\n\\nresponse = requests.post(API_URL, json={\"prompt\": PROMPT}, headers={\"X-API-KEY\": API_KEY})\\nprint(response.json())\\n```\\n\\n#### Conclusion\\n\\nBy setting up a secure API to control access to your AI model, you ensure that only authorized users can interact with your model, and you can manage their usage to avoid unnecessary costs. This setup can be further extended by integrating user authentication, user roles, and more advanced credit systems to handle larger-scale applications.\\n\\nRemember, while this example is basic, the principles can be extended to create more sophisticated systems, depending on your application needs. Safety and security are paramount, and by managing access through your own backend server, you take a significant step in securing your AI model usage.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 982, 'prompt_tokens': 5683, 'total_tokens': 6665, 'completion_time': 4.91, 'prompt_time': 0.29367471, 'queue_time': 0.474498129, 'total_time': 5.20367471}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-20d0542c-878b-47a9-953b-f2baf126a5b4-0', usage_metadata={'input_tokens': 5683, 'output_tokens': 982, 'total_tokens': 6665})}\n",
      "{'video_url': 'https://www.youtube.com/watch?v=cy6EAp4iNN4', 'transcript': \"AI models are powerful tools but in\\norder to use them properly and securely\\nyou need to control them using an API so\\nin today's video I'm going to show you\\nhow to write a very simple python API to\\ncontrol access to an llm or an AI model\\nnow first I want to explain why you\\nactually need to do this you understand\\nthe security and the importance of\\nsetting this up now let's say that you\\nwant to use an llm something like chat\\nGPT or something like deep seek right if\\nthat's the case what you're probably\\ngoing to do is you're either going to\\nattempt to run this locally or you're\\ngoing to use the cloud provider so\\nyou're going to go to deepseeker open\\naai you're going to generate an API key\\nand then you're going to take that API\\nkey and that's what you send anytime you\\nwant to make a request or use the llm\\nnow this is great and it works well if\\nyou're doing this locally but in a\\nproduction environment if you were to\\ntake this API key and you were to use it\\ndirectly from your own front end so\\nsomething like a website or a mobile\\napplication you'd be introducing a huge\\nsecurity risk\\nthe reason why that's the case is\\nbecause if you use one of these keys\\nfrom your front end then anyone who has\\naccess to your front end code will be\\nable to see and use that key so they\\ncould really take advantage of that and\\nthey could send all kinds of requests\\nand cost you a ton of money because\\nanytime you send a request to something\\nlike a cloud provider will it cost you\\nmoney and even if you're running this\\nlocally on your own computer it costs\\nyou compute time or resources so the\\nmain idea here is that you only want to\\ninvoke an llm especially if it's it's\\ncoming from a provider like open aai\\nfrom something that's secure something\\nthat you control which would be a\\nbackend server or an API the basic flow\\nwould be if someone has access to your\\nfront-end application they would then\\nsend a request to your own backend from\\nyour backend you could then control\\nwhether or not you wanted to send a\\nrequest to the llm so you can control\\nroughly how much it will cost and which\\nof your users are allowed to use the llm\\nthis is common practice from all of the\\nAI applications that you've used before\\nand you'll typically notice that you get\\nsomething like credits so maybe you have\\n10 credits where you're able to call an\\nllm 10 times and then after that you\\nwould need to pay money to the service\\nprovider to compensate them for how much\\nthe llm is costing hopefully that makes\\na little bit of sense but the basic idea\\nhere is that we need to be able to\\ncontrol the calls to our llm so that we\\ncan decide which users can call it or\\nnot call it and make sure that it\\ndoesn't cost us too much money so with\\nthat in mind let's get into it and start\\nbuilding this out so like I mentioned\\nyou can control access to any llm that\\nyou want and you can use something like\\nopen AI or deep seek but in my case I'm\\njust going to run an llm locally on my\\nown computer using something called AMA\\nthis is completely free it's open source\\nand it lets you run models on your own\\nmachine assuming you have good enough\\nHardware I have an entire video on how\\nto set this up so I'll leave it on\\nscreen but I'll give you the cliff notes\\nHere what you need to do is simply\\ndownload ama if you want to set this up\\non your own computer so once you've\\ndownloaded and installed this you can\\nopen up a ter teral or a command prompt\\nand inside of here you can just type AMA\\nto make sure this is working if for some\\nreason the command isn't recognized you\\ncan try running olama by just typing the\\nname of the application if you're on\\nsomething like Windows and double\\nclicking it to run now once olama is\\nworking you can pull an olama model that\\nyou want to run locally now to do that\\nyou type a llama pull and then you put\\nthe name of the model in this case I'll\\nuse a model like mistro but you can use\\nmodels like llama 3 and any open source\\nmodel really you want you can pull it\\nand run it locally assuming you have\\nsufficient hardware and all of the\\nhardware requirements are specified on\\nthe olama website where it lists all of\\nthe different open- Source models so I'm\\ngoing to pull the mistal model I already\\nhave this downloaded so now it's on my\\nmachine and then if I wanted to use this\\nmodel I could type llama run and then\\nmistl and then I can just start chatting\\nwith the model like I would in any other\\ncase now to leave I can type slash bu\\nand that's great and now we'll be able\\nto use AMA from our python code again I\\nhave an entire video in case you're\\nconfused or more help that I'll leave on\\nscreen okay so now that we've done that\\nwhat we're going to do is start setting\\nup a basic API to use our llm model now\\nagain you can use any llm that you want\\nhere the important thing is that we just\\nset up the API and we secure it properly\\nso for this video the IDE that I'm going\\nto be using is pycharm now this is one\\nof the best idees when it comes to\\nworking with python especially for\\nthings like apis and dealing with\\nFrameworks like Fast API Jango or flask\\nwhich we'll be using in this video now I\\nhave a long-term partnership with py and\\nif you guys want to get access to the\\nprofessional Edition with an extended\\nfree trial of up to 3 months you can do\\nthat by clicking the link in the\\ndescription py charm has two versions\\nThe Community Edition which is\\ncompletely free and the professional\\nEdition which you do need to pay for but\\nagain because you're a viewer of this\\nchannel I can give it to you 3 months\\nfor free so you can try it out and see\\nif you like it it has all kinds of great\\nfeatures and we can actually test our\\nAPI directly from the IDE which makes\\nour life a lot easier and you'll see\\nthat in this video again links in the\\ndescription so first things first we're\\njust going to set up the dependencies\\nfor our python project to build this API\\nso I'm going to make a new file I just\\nopened a folder here in this IDE so I\\nwent up here and I just opened a folder\\nI called it API for llm and then from\\nhere I'm going to go new file and then\\nI'm going to call this\\nrequirements.txt\\nokay from this requirements.txt file I'm\\njust going to specify the dependencies\\nthat I'll need for my python project and\\nobviously you need python installed in\\norder to follow along with the rest of\\nthese steps so I'm going to install fast\\nAPI UV corn olama Python\\nd.v and then requests okay now fast API\\nis what we'll use for building our API\\nit'll be very simple don't worry uicorn\\nis for running our fast API application\\nolama is for interfacing with olama to\\nuse a local llm and then python. EnV is\\nfor loading in an environment variable\\nfile and request is for Center request\\nwhen we later test our API now we've\\nspecified these in our requirements.txt\\nso the next step is to Simply install\\nthem so what we're going to do is just\\nopen up a new terminal instance I just\\nhave one here and make sure it's in the\\nsame folder where your requirements.txt\\nfile is if you're opening the terminal\\nfrom an IDE or an editor then it should\\njust already be in the correct directory\\nfrom here assuming we have python\\ninstalled we're going to type pip\\ninstall - R and then requirements.txt\\nnow this is just going to install all of\\nthe dependencies into our main python\\ninstallation if you want you can use a\\nvirtual environment but I'm not going to\\ncover that in the name of time if that\\ncommand doesn't work you can try pip 3\\ninstall - R requirements.txt and what\\nthis does is install all of the\\ndependencies from the requirements file\\nnow that we have all of that installed\\nwe can start writing some simple code\\nand get an API up and running so what\\nI'm going to do is make a new file and\\nit's going to be a python file and I'm\\njust going to call this main.py you can\\ncall it anything you want but I\\nrecommend naming it main just to stick\\nwith the um kind of conventions I'm\\nusing in this tutorial it'll be a little\\nbit easier to follow along with okay so\\nfrom here we're going to make a very\\nsimple API and to do that we're going to\\nsay from Fast API import and then fast\\nAPI and then we're going to import o\\nLama okay then we're going to make an\\napp so we're going to say app is equal\\nto fast API and we're going to Define an\\nendpoint an endpoint is just a URL on\\nthis server that we can access so for\\nnow we're going to say ATA dopost and\\nwe're going to do slash generate now you\\ncan call this anything that you want but\\nwhat this specifies is that you need to\\nsend a post request which is a type of\\nHTTP request to this URL and then the\\nfunction that we write which will be\\ncalled generate will run whenever we go\\nto this/ generate rout now inside of\\nhere we're going to take a prompt which\\nis a string and what this specifies is\\nthat you need to pass a query parameter\\nwhich looks like this prompt is equal to\\nand then whatever the prompt is whenever\\nyou want to call this rout so we know\\nwhat the prompt to the llm should be\\nfrom here we can generate a simple\\nresponse by saying response is equal to\\nol. chat from the chat we're going to\\nsay the model is equal to whatever model\\nwe pulled and we want to use in this\\ncase it's mistal then we're going to say\\nmessages is equal to we're going to\\nspecify a list and we're going to put a\\npython dictionary in the python\\ndictionary we need to specify the role\\nas user and then the content as The\\nPrompt okay so you can pass multiple\\nmessages here but if you just want to\\npass one then you just do it like this\\nand obviously you can modify the call to\\nthe llm but I'm just keeping it basic\\nfor right now after here we're going to\\nreturn the response and the way we'll do\\nthis is we'll return a python dictionary\\nthat has response and then the response\\nand this is going to be the message and\\nthe content just so that we strip out\\nthe information that we actually want so\\nthat's literally it we're saying okay we\\nwant to chat with AMA this works because\\nwe've downloaded on our machine we\\nspecify the model we want to use the\\nmessages that we want to pass you could\\nalso pass a system message if you wanted\\nto customize the prompt or something and\\nthen you can have a response we say\\nresponse message content okay now we\\nneed to run the API so to run the API we\\ncan open up our terminal and make sure\\nagain you're in the same directory where\\nyour python script is and you can type\\nuicorn and then main colon app-- reload\\nnow what these variables are right here\\nmain is the name of my file so main.py\\nand then app is the name of my fast API\\napplication so app so if you've changed\\neither of those you need to adjust these\\nright here and the D- reload will run\\nthis in development mode so it will\\nreload the server anytime any changes\\nare made so if I hit enter here you can\\nsee that now the API is running it tells\\nus what port it's running on so Port\\n8000 on Local Host and now we need to\\ntest it now there's various ways to test\\nthe API but if you're working inside of\\npy charm then you can open up this\\nendpoints tab you can find it by\\nclicking on these three dots and look\\nfor it here or you can see kind of a\\ncircle inside of a half circle from here\\nyou're going to specify the main. apppp\\npackage okay you're going to press the\\nslash generate so it can automatically\\nactually read um the endpoint and then\\nwhat you can do is see the kind of\\nsample request right here so you can see\\nit says post local hostport 8000 SL\\ngenerate and then you'll need to add\\nthis query parameter question mark\\nprompt equal to and then whatever you\\nwant to prompt the llm with once you do\\nthat you can hit submit request you can\\nwait for this to run it will take a\\nsecond and then you can see the response\\nright here so let me just make this a\\nlittle bit bigger so that you guys can\\nread this and you can see that we get\\nthe response from the llm it says hello\\nworld it's a classic example blah blah\\nblah blah blah and you can use it as you\\nsee fit now other than that the easiest\\nway to test this if you're not using\\nsomething like py charm is to download a\\ntool called postmen so I'm just going to\\nopen it up right here this is a free\\ntool that you can download on your\\ncomputer whether it's window Mac Linux\\never that allows you to test your apis\\nreally easily so if you don't have\\nfamiliarity testing them simply download\\nand install this tool and then open it\\nup and I'll show you how to use it so\\nassuming our API is still running in our\\nterminal we can open up the postman\\napplication from here we can press this\\nlittle plus button you might need to\\nmake an account or it might ask you for\\nsome stuff you don't need to but just\\nget into this workspace press on plus\\nand then what you're going to do is\\nchange get here to say post because\\nwe're sending a post request from here\\nyou're going to type in the URL so HTTP\\ncolon colon SL localhost SL generate\\nokay and we need to make sure this is\\nPort 8000 so sorry Local Host Port 8000\\nSL generate question mark prompt is\\nequal to whatever you want to prompt so\\nsomething like hello world and then what\\nwe can do is we can press on send\\nPostman will automatically send the\\nrequest for us again you just need to\\nmake sure you have a URL that looks like\\nthis when we do this you can then see\\nthat we get the response back from the\\nllm\\nokay so now we've written a basic llm\\nbut we want to make this secure we want\\nto add some kind of authorization or at\\nleast some logic so that not anyone can\\njust use this so what I'm going to do\\nnow is adjust this code so that only\\nsomeone that has access to our function\\nor to our API will be able to use the\\nllm because after all that's the entire\\npoint of this we don't want to let just\\na random person use the llm we want to\\nmaybe control their access okay so now\\nlet's add our own version of an API key\\nnow this is a key that someone will need\\nto send to our backend server if they\\nwant to utilize the API now what we can\\ndo with this key is we can just give it\\nto certain users or we can generate it\\nhowever we want in any way that we see\\nfit and we can have some kind of value\\nattached to this key to limit the number\\nof requests that someone could send so\\nfor example any API key that I give to\\nsomeone maybe it only has a limit of 10\\nso after you've used the key 10 times\\nmaybe you need to pay more money or you\\nneed to wait a day or something\\nsomething like that before you could use\\nthe key again I'm going to set up some\\nbasic logic but it will give you a sense\\nof how to do this so what I'm going to\\ndo is I'm going to import a few things\\nso from Fast API I'm going to import\\ndepends HTTP exception and header okay\\nI'm then going to import OS and I'm\\ngoing to say from.\\nEnV import load. EnV now I'm going to\\ncall the load. EnV function and what\\nthis is going to do is load a value from\\nour environment variable iable file\\nwhich I'm going to create right now so\\ninside of our directory we're going to\\nmake a new file and we're going to call\\nthis EnV okay inside of here we're going\\nto say API key is equal to and then we\\ncan set this to some secret key that we\\nwould only give people that we want to\\nhave access to our API in order to use\\nthe llm now you could have as many API\\nKeys as you want and typically you would\\ngenerate these for users so when people\\nhave an account they would sign in and\\nthen you could issue them an API key and\\nthey need to use that API key to access\\nthe API that's a more advanced topic and\\nI'll leave a video on screen that shows\\nexactly how to do that but you'll get\\nthe idea with what I show you right here\\nso in this EnV file we've specified a\\nvariable called secret key now what\\nwe're going to do is we're going to load\\nin that value so we're going to say API\\nKeys is equal to a list or sorry is\\nequal to a set and inside of here we're\\ngoing to say os. getv and then the API\\nkey now this works because we've l\\nloaded in the environment variable file\\nand now we can get that environment\\nvariable and we put that in our list of\\nvalid API keys and if we wanted to we\\ncould associate this with a value like\\n10 and then we could subtract from this\\nvalue anytime someone uses the API and\\ntreat this as like their credits so I\\ncan say you know this key has five\\ncredits remaining so we could change\\nthis to API key credits or something and\\nyou know what let's go ahead and do that\\nokay now that we've done that we're\\ngoing to write a simple function that\\nwill verify that a user has an API key\\nbefore we allow them to continue so I'm\\ngoing to say Define verify uncore aior\\nkey and we're going to specify xcore\\naior key we're going to say this is a\\nstring and this is equal to header and\\nthen none okay now what this is going to\\ndo is it's going to look in the headers\\nof our request which I'll show you in\\none second for a variable called X API\\nkey if finds it it's going to associate\\nit with this parameter right here and\\nthen allow us to read it in and use it\\ninside of this um what do you call it\\nfunction so what we're actually going to\\ndo here is we're going to say credits is\\nequal to API key credits. getet xapi key\\nnow we're going to specify the default\\nvalue is zero and what this means is\\nthat we're going to attempt to get\\nwhatever the number of credits are\\nassociated with our API key but if that\\nAPI key doesn't exist then we're just\\ngoing to get zero then we're simply\\ngoing to check if the credits are equal\\nto zero so we're going to say if the\\ncredits are less than or equal to zero\\nthen what we're going to do is we're\\ngoing to return or sorry we're going to\\nraise an HTTP exception and we're going\\nto say status 401 and then we're going\\nto say invalid API key or no credits\\nokay just telling them hey you don't\\nhave any credits left or your API key is\\nexpired or actually from here we can\\njust return the X API key so if they do\\nhave a valid key we can return it so we\\ncan use it later on and then inside of\\nhere we're going to we're going to make\\nsure that they have an API key and that\\nit's valid before we allow them to use\\nthis function so to do that inside of\\nthe parameters we're going to add X _\\nAPI key we're going to say this is type\\nstring and this is equal to depends on\\nverify API key now just let me use this\\nvariable and then it will light up so\\nyou can read it easier but what this\\nmeans is that we're depending on this\\nfunction call and then if this function\\ncall raises an error we just won't do\\nanything inside of here whereas if it\\nReturns the API key then we're good to\\ngo and we'll enter this function so\\ninside of here the first thing we're\\ngoing to do is just subtract our credits\\nso we're going to say API key credits at\\nX API key and then we're just going to\\nsay minus equals 1 so we're just going\\nto subtract one from that to specify hey\\nwe're removing one of your credits\\nbecause now you've successfully used\\nthis function and that's it that's\\nliterally all that we need and now we've\\nadded kind of control to this API where\\nyou can only use this if you pass us an\\nAPI key now again we've just hardcoded\\nan API key here from our environment\\nvariable file F but you could\\ndynamically generate these however you\\nwanted to and you can handle them for\\nvarious users so I'm showing you the\\nlogic but obviously you would need to go\\na step further with this to have correct\\nauthentication and authorization for a\\nlarger Scale app so now that we've done\\nthis we want to test the authorized API\\nso we should be able to rerun this so\\nhere it looks like it was rerunning but\\nI'm just going to shut it down and\\nrestart it to make sure it loads in the\\nenvironment variable so same thing\\nuvicorn main app reload so now to test\\nthe API again there's multiple ways to\\ndo that but I'm just going to use\\nPostman so that it's universally\\navailable for all of you and I'm going\\nto keep the same request that I had last\\ntime except this time I'm going to go\\ninto headers now inside of headers here\\nI'm just going to add the header for my\\nAPI key so I'm going to add the header\\nof xcore API uncore key like that I\\nbelieve that's what we called it and\\nthen for the value I'm going to say\\nsecret key okay all right so a silly\\nmistake here but actually we want to use\\nuh High when we specify this in the\\nheader not underscores so just x-\\napi-key and then put the value secret\\nkey you don't need any quotes and now\\nwhen we send this you should see that it\\nworks it just takes a second and then we\\nget the response back and if we try to\\ndo this now five times let's just keep\\ndoing it so that our credits go lower\\nand lower you should see eventually when\\nwe get to the fifth request that it\\ndoesn't work and it tells us that we run\\nout of tokens and you can see that's\\nwhat we get it says invalid API key or\\nno credits so there you go we've now\\nbuilt an authenticated API that allows\\nus to utilize our llm with this API key\\nnow again you want to manage these Keys\\ncorrectly there's a lot of other logic\\nand things you can do and I will leave a\\nvideo on screen that will help you do\\nthat now beyond that I'm quickly just\\ngoing to show you a python script that\\nyou can use if you wanted to test the\\nAPI from python code so I'm just going\\nto make a script called test API now\\nwhat I'm going to do is just copy in\\nsome code so here is the code you can\\nsee that I import requests I import the\\ncode to load my environment variable\\nfile I specify the URL that I want to\\nsend my request to and the prompt that I\\nwant to pass to the llm I then specify\\nmy headers so I have my API key which is\\nequal to what I'm loading from the\\nenvironment variable file content type\\napplication Json send a post request\\nwith my URL and my headers and then I\\nprint out the response. Json now I just\\nrestarted my server I took a quick cut\\nso when I restarted the server it will\\nreset the amount of credits that I have\\nand now if I run this file just takes a\\nsecond and we should see that we get our\\nresponse and we do you can see that I\\nget the response back from the llm okay\\nso that's pretty much all I wanted to\\nshow you in this video I know there was\\na lot but I wanted to explain how you\\nset up a simple API with this API token\\nwhich you can then use to control access\\nto the llm the llm call here can be\\nanything that you want but it's\\nimportant that you do gate it and you do\\ncontrol it from a backend server so that\\nfront-end users can't directly access\\nyour tokens now you may be asking well\\nwhat about this token right here the\\nwhole point of this token is that you\\ncontrol this token rather than open AI\\nor deep seek or something like that so\\nnow if you want to invalidate the token\\nif you want to have a certain number of\\ncredits if you only want to give it to\\npremium users or paid users you can do\\nthat right you can control how you want\\nto issue this token but if you just use\\nan openai token now anyone that has\\naccess to that can go and utilize open\\naai and cost you a ton of money so\\nthat's why you want to set up this own\\nkind of custom back end where you have\\nfull control as the developer and the\\napp owner I will leave this code in the\\ndescription in case you want to check it\\nout it'll be available from GitHub if\\nyou enjoyed the video make sure to leave\\na like subscribe to the channel and I\\nwill see you in the next one\\n[Music]\", 'blog': AIMessage(content='### Controlling Access to AI Models with a Secure Python API\\n\\nAI models have revolutionized the way we interact with data and automate tasks, but with great power comes great responsibility. Ensuring these models are used securely and responsibly is crucial, especially when it comes to managing costs and user access. In this blog post, I’ll guide you through creating a secure Python API to control access to an AI model, using a local language model (LLM) via a tool called AMA. This method allows you to manage who can access your AI model, ensuring your resources aren’t abused.\\n\\n#### Why Control Access?\\n\\nAccess control is paramount when integrating AI models into production environments. For instance, using a model like OpenAI’s GPT or DeepSeek’s DeepSeeker directly from a front-end application introduces significant security risks. If you embed an API key directly into your front-end code, anyone can potentially access and misuse it, leading to unauthorized usage and significant financial costs.\\n\\nTo mitigate this, you should control the model\\'s access from a backend server, where you can manage requests, user permissions, and resource costs.\\n\\n#### Setting Up the Environment\\n\\nBefore diving into the code, you need to have your environment ready to run a local LLM. Using AMA, a tool for running language models locally, you can achieve this without the need for cloud-based resources. You’ll need to install AMA and pull a model to run locally.\\n\\n1. **Install AMA**: Follow the instructions on the AMA GitHub or documentation to install it.\\n2. **Pull a Model**: Use the command `ama pull mistral` to download and set up a model like Mistral.\\n\\n#### Building the API\\n\\nTo build the API, you\\'ll use FastAPI, a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\\n\\n1. **Setup Dependencies**: Create a `requirements.txt` file with the dependencies (`fastapi`, `uvicorn`, `olama`, `python-dotenv`, `requests`).\\n2. **Install Dependencies**: Use `pip install -r requirements.txt` to install the necessary libraries.\\n3. **Create the API Script**: Create a `main.py` file where the magic happens.\\n\\n```python\\nfrom fastapi import FastAPI, Depends, HTTPException\\nfrom olama import chat\\nimport os\\nfrom dotenv import load_dotenv\\nimport requests\\n\\napp = FastAPI()\\n\\nload_dotenv()\\nAPI_KEY = os.getenv(\\'API_KEY\\')\\nAPI_KEY_CREDITS = {API_KEY: 5}  # Setting up credits for the API key\\n\\ndef verify_api_key(x_api_key: str = Depends()):\\n    credits = API_KEY_CREDITS.get(x_api_key, 0)\\n    if credits <= 0:\\n        raise HTTPException(status_code=401, detail=\"Invalid API key or no credits\")\\n    API_KEY_CREDITS[x_api_key] -= 1\\n    return x_api_key\\n\\n@app.post(\"/generate\")\\ndef generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\\n    response = chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\\n    return {\"response\": response}\\n\\nif __name__ == \"__main__\":\\n    import uvicorn\\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\\n```\\n\\n#### Testing the API\\n\\nTo test the API, you can use a tool like Postman or write a simple Python script.\\n\\n1. **Using Postman**: Set up a POST request to `http://localhost:8000/generate` with the header `X-API-KEY` and the value of your secret key.\\n\\n2. **Python Script**: Here’s a simple script to test your API:\\n\\n```python\\nimport requests\\nfrom dotenv import load_dotenv\\nimport os\\n\\nload_dotenv()\\n\\nAPI_URL = \"http://localhost:8000/generate\"\\nAPI_KEY = os.getenv(\\'API_KEY\\')\\nPROMPT = \"What is the meaning of life?\"\\n\\nresponse = requests.post(API_URL, json={\"prompt\": PROMPT}, headers={\"X-API-KEY\": API_KEY})\\nprint(response.json())\\n```\\n\\n#### Conclusion\\n\\nBy setting up a secure API to control access to your AI model, you ensure that only authorized users can interact with your model, and you can manage their usage to avoid unnecessary costs. This setup can be further extended by integrating user authentication, user roles, and more advanced credit systems to handle larger-scale applications.\\n\\nRemember, while this example is basic, the principles can be extended to create more sophisticated systems, depending on your application needs. Safety and security are paramount, and by managing access through your own backend server, you take a significant step in securing your AI model usage.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 982, 'prompt_tokens': 5683, 'total_tokens': 6665, 'completion_time': 4.91, 'prompt_time': 0.29367471, 'queue_time': 0.474498129, 'total_time': 5.20367471}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-20d0542c-878b-47a9-953b-f2baf126a5b4-0', usage_metadata={'input_tokens': 5683, 'output_tokens': 982, 'total_tokens': 6665}), 'review': AIMessage(content='### Review of \"Controlling Access to AI Models with a Secure Python API\"\\n\\nThe blog post titled \"Controlling Access to AI Models with a Secure Python API\" is a well-crafted guide that addresses a critical aspect of AI model deployment: security and access control. The post is structured in a logical manner, providing a clear breakdown of the need for access control, the setup process, and an implementation example using Python and FastAPI.\\n\\n#### Strengths:\\n\\n1. **Comprehensive Explanation of Importance:**\\n   - The post begins with a thorough explanation of why controlling access to AI models is crucial, particularly in production environments. It highlights the security risks associated with direct API key embedding in front-end applications and the potential for misuse, which is an important point for developers to consider.\\n\\n2. **Practical and Detailed Instructions:**\\n   - The instructions for setting up the environment with AMA (a tool for running language models locally) are detailed and straightforward, providing a clear path for readers to follow along.\\n   - The API building section is particularly robust, with step-by-step guidance on how to set up the dependencies, create the API script, and even handle user authentication and credit management through API keys.\\n\\n3. **Code Examples:**\\n   - The inclusion of code snippets not only enhances the practicality of the post but also makes it easier for readers to understand and implement the solution. The FastAPI-based API script is well-commented and provides a good starting point for implementing a secure API for AI models.\\n\\n4. **Testing the API:**\\n   - The post includes methods for testing the API using both Postman and a Python script, which is useful for readers who want to verify the functionality of their setup. This approach caters to a broader audience, from those preferring GUI-based testing to those who prefer scripting.\\n\\n#### Areas for Improvement:\\n\\n1. **Advanced Security Practices:**\\n   - While the post covers the basics of API key verification and credit management, it could delve deeper into advanced security practices such as encryption, rate limiting, and more refined authentication mechanisms. This would provide a more comprehensive security framework for handling AI models.\\n\\n2. **Scalability and Performance Considerations:**\\n   - The post could benefit from a section discussing how to scale the API and handle performance issues that might arise with higher user counts. This could include strategies for load balancing, caching, and optimizing API responses, which are essential for production environments.\\n\\n3. **Deployment and Monitoring:**\\n   - Although the post focuses on building the API, a brief section on how to deploy such an API in a production environment (e.g., Docker, Kubernetes) and how to monitor its performance and security could add significant value.\\n\\nOverall, \"Controlling Access to AI Models with a Secure Python API\" is a valuable resource for developers looking to secure their AI models and manage user access responsibly. The post effectively balances theory with practical implementation, making it an excellent guide for both beginners and experienced developers in the field of AI and API development.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 608, 'prompt_tokens': 1315, 'total_tokens': 1923, 'completion_time': 3.04, 'prompt_time': 0.06315672, 'queue_time': 0.238658062, 'total_time': 3.10315672}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-0de595a5-5213-4e1d-b6f1-3b25c367a20e-0', usage_metadata={'input_tokens': 1315, 'output_tokens': 608, 'total_tokens': 1923})}\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "### Controlling Access to AI Models with a Secure Python API\n",
       "\n",
       "AI models have revolutionized the way we interact with data and automate tasks, but with great power comes great responsibility. Ensuring these models are used securely and responsibly is crucial, especially when it comes to managing costs and user access. In this blog post, I’ll guide you through creating a secure Python API to control access to an AI model, using a local language model (LLM) via a tool called AMA. This method allows you to manage who can access your AI model, ensuring your resources aren’t abused.\n",
       "\n",
       "#### Why Control Access?\n",
       "\n",
       "Access control is paramount when integrating AI models into production environments. For instance, using a model like OpenAI’s GPT or DeepSeek’s DeepSeeker directly from a front-end application introduces significant security risks. If you embed an API key directly into your front-end code, anyone can potentially access and misuse it, leading to unauthorized usage and significant financial costs.\n",
       "\n",
       "To mitigate this, you should control the model's access from a backend server, where you can manage requests, user permissions, and resource costs.\n",
       "\n",
       "#### Setting Up the Environment\n",
       "\n",
       "Before diving into the code, you need to have your environment ready to run a local LLM. Using AMA, a tool for running language models locally, you can achieve this without the need for cloud-based resources. You’ll need to install AMA and pull a model to run locally.\n",
       "\n",
       "1. **Install AMA**: Follow the instructions on the AMA GitHub or documentation to install it.\n",
       "2. **Pull a Model**: Use the command `ama pull mistral` to download and set up a model like Mistral.\n",
       "\n",
       "#### Building the API\n",
       "\n",
       "To build the API, you'll use FastAPI, a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints.\n",
       "\n",
       "1. **Setup Dependencies**: Create a `requirements.txt` file with the dependencies (`fastapi`, `uvicorn`, `olama`, `python-dotenv`, `requests`).\n",
       "2. **Install Dependencies**: Use `pip install -r requirements.txt` to install the necessary libraries.\n",
       "3. **Create the API Script**: Create a `main.py` file where the magic happens.\n",
       "\n",
       "```python\n",
       "from fastapi import FastAPI, Depends, HTTPException\n",
       "from olama import chat\n",
       "import os\n",
       "from dotenv import load_dotenv\n",
       "import requests\n",
       "\n",
       "app = FastAPI()\n",
       "\n",
       "load_dotenv()\n",
       "API_KEY = os.getenv('API_KEY')\n",
       "API_KEY_CREDITS = {API_KEY: 5}  # Setting up credits for the API key\n",
       "\n",
       "def verify_api_key(x_api_key: str = Depends()):\n",
       "    credits = API_KEY_CREDITS.get(x_api_key, 0)\n",
       "    if credits <= 0:\n",
       "        raise HTTPException(status_code=401, detail=\"Invalid API key or no credits\")\n",
       "    API_KEY_CREDITS[x_api_key] -= 1\n",
       "    return x_api_key\n",
       "\n",
       "@app.post(\"/generate\")\n",
       "def generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\n",
       "    response = chat(model=\"mistral\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
       "    return {\"response\": response}\n",
       "\n",
       "if __name__ == \"__main__\":\n",
       "    import uvicorn\n",
       "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
       "```\n",
       "\n",
       "#### Testing the API\n",
       "\n",
       "To test the API, you can use a tool like Postman or write a simple Python script.\n",
       "\n",
       "1. **Using Postman**: Set up a POST request to `http://localhost:8000/generate` with the header `X-API-KEY` and the value of your secret key.\n",
       "\n",
       "2. **Python Script**: Here’s a simple script to test your API:\n",
       "\n",
       "```python\n",
       "import requests\n",
       "from dotenv import load_dotenv\n",
       "import os\n",
       "\n",
       "load_dotenv()\n",
       "\n",
       "API_URL = \"http://localhost:8000/generate\"\n",
       "API_KEY = os.getenv('API_KEY')\n",
       "PROMPT = \"What is the meaning of life?\"\n",
       "\n",
       "response = requests.post(API_URL, json={\"prompt\": PROMPT}, headers={\"X-API-KEY\": API_KEY})\n",
       "print(response.json())\n",
       "```\n",
       "\n",
       "#### Conclusion\n",
       "\n",
       "By setting up a secure API to control access to your AI model, you ensure that only authorized users can interact with your model, and you can manage their usage to avoid unnecessary costs. This setup can be further extended by integrating user authentication, user roles, and more advanced credit systems to handle larger-scale applications.\n",
       "\n",
       "Remember, while this example is basic, the principles can be extended to create more sophisticated systems, depending on your application needs. Safety and security are paramount, and by managing access through your own backend server, you take a significant step in securing your AI model usage."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Review of \"Controlling Access to AI Models with a Secure Python API\"\n",
       "\n",
       "The blog post titled \"Controlling Access to AI Models with a Secure Python API\" is a well-crafted guide that addresses a critical aspect of AI model deployment: security and access control. The post is structured in a logical manner, providing a clear breakdown of the need for access control, the setup process, and an implementation example using Python and FastAPI.\n",
       "\n",
       "#### Strengths:\n",
       "\n",
       "1. **Comprehensive Explanation of Importance:**\n",
       "   - The post begins with a thorough explanation of why controlling access to AI models is crucial, particularly in production environments. It highlights the security risks associated with direct API key embedding in front-end applications and the potential for misuse, which is an important point for developers to consider.\n",
       "\n",
       "2. **Practical and Detailed Instructions:**\n",
       "   - The instructions for setting up the environment with AMA (a tool for running language models locally) are detailed and straightforward, providing a clear path for readers to follow along.\n",
       "   - The API building section is particularly robust, with step-by-step guidance on how to set up the dependencies, create the API script, and even handle user authentication and credit management through API keys.\n",
       "\n",
       "3. **Code Examples:**\n",
       "   - The inclusion of code snippets not only enhances the practicality of the post but also makes it easier for readers to understand and implement the solution. The FastAPI-based API script is well-commented and provides a good starting point for implementing a secure API for AI models.\n",
       "\n",
       "4. **Testing the API:**\n",
       "   - The post includes methods for testing the API using both Postman and a Python script, which is useful for readers who want to verify the functionality of their setup. This approach caters to a broader audience, from those preferring GUI-based testing to those who prefer scripting.\n",
       "\n",
       "#### Areas for Improvement:\n",
       "\n",
       "1. **Advanced Security Practices:**\n",
       "   - While the post covers the basics of API key verification and credit management, it could delve deeper into advanced security practices such as encryption, rate limiting, and more refined authentication mechanisms. This would provide a more comprehensive security framework for handling AI models.\n",
       "\n",
       "2. **Scalability and Performance Considerations:**\n",
       "   - The post could benefit from a section discussing how to scale the API and handle performance issues that might arise with higher user counts. This could include strategies for load balancing, caching, and optimizing API responses, which are essential for production environments.\n",
       "\n",
       "3. **Deployment and Monitoring:**\n",
       "   - Although the post focuses on building the API, a brief section on how to deploy such an API in a production environment (e.g., Docker, Kubernetes) and how to monitor its performance and security could add significant value.\n",
       "\n",
       "Overall, \"Controlling Access to AI Models with a Secure Python API\" is a valuable resource for developers looking to secure their AI models and manage user access responsibly. The post effectively balances theory with practical implementation, making it an excellent guide for both beginners and experienced developers in the field of AI and API development."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "# Run the graph until the first interruption\n",
    "for event in workflow.stream({\"video_url\": \"https://www.youtube.com/watch?v=cy6EAp4iNN4\"}, thread, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    blog = event.get(\"blog\", None)\n",
    "    review = event.get(\"review\", None)\n",
    "    if blog is not None and review is not None:\n",
    "        display(Markdown(blog.content))\n",
    "        display(Markdown(review.content))\n",
    "        break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Awaiting human feedback...\n"
     ]
    }
   ],
   "source": [
    "user_feedback = input(\"Please provide your feedback:\")\n",
    "if user_feedback:\n",
    "    workflow.invoke(Command(resume=user_feedback), thread)\n",
    "else:\n",
    "    workflow.invoke(Command(resume=\"\"), thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'video_url': 'https://www.youtube.com/watch?v=cy6EAp4iNN4',\n",
       " 'transcript': \"AI models are powerful tools but in\\norder to use them properly and securely\\nyou need to control them using an API so\\nin today's video I'm going to show you\\nhow to write a very simple python API to\\ncontrol access to an llm or an AI model\\nnow first I want to explain why you\\nactually need to do this you understand\\nthe security and the importance of\\nsetting this up now let's say that you\\nwant to use an llm something like chat\\nGPT or something like deep seek right if\\nthat's the case what you're probably\\ngoing to do is you're either going to\\nattempt to run this locally or you're\\ngoing to use the cloud provider so\\nyou're going to go to deepseeker open\\naai you're going to generate an API key\\nand then you're going to take that API\\nkey and that's what you send anytime you\\nwant to make a request or use the llm\\nnow this is great and it works well if\\nyou're doing this locally but in a\\nproduction environment if you were to\\ntake this API key and you were to use it\\ndirectly from your own front end so\\nsomething like a website or a mobile\\napplication you'd be introducing a huge\\nsecurity risk\\nthe reason why that's the case is\\nbecause if you use one of these keys\\nfrom your front end then anyone who has\\naccess to your front end code will be\\nable to see and use that key so they\\ncould really take advantage of that and\\nthey could send all kinds of requests\\nand cost you a ton of money because\\nanytime you send a request to something\\nlike a cloud provider will it cost you\\nmoney and even if you're running this\\nlocally on your own computer it costs\\nyou compute time or resources so the\\nmain idea here is that you only want to\\ninvoke an llm especially if it's it's\\ncoming from a provider like open aai\\nfrom something that's secure something\\nthat you control which would be a\\nbackend server or an API the basic flow\\nwould be if someone has access to your\\nfront-end application they would then\\nsend a request to your own backend from\\nyour backend you could then control\\nwhether or not you wanted to send a\\nrequest to the llm so you can control\\nroughly how much it will cost and which\\nof your users are allowed to use the llm\\nthis is common practice from all of the\\nAI applications that you've used before\\nand you'll typically notice that you get\\nsomething like credits so maybe you have\\n10 credits where you're able to call an\\nllm 10 times and then after that you\\nwould need to pay money to the service\\nprovider to compensate them for how much\\nthe llm is costing hopefully that makes\\na little bit of sense but the basic idea\\nhere is that we need to be able to\\ncontrol the calls to our llm so that we\\ncan decide which users can call it or\\nnot call it and make sure that it\\ndoesn't cost us too much money so with\\nthat in mind let's get into it and start\\nbuilding this out so like I mentioned\\nyou can control access to any llm that\\nyou want and you can use something like\\nopen AI or deep seek but in my case I'm\\njust going to run an llm locally on my\\nown computer using something called AMA\\nthis is completely free it's open source\\nand it lets you run models on your own\\nmachine assuming you have good enough\\nHardware I have an entire video on how\\nto set this up so I'll leave it on\\nscreen but I'll give you the cliff notes\\nHere what you need to do is simply\\ndownload ama if you want to set this up\\non your own computer so once you've\\ndownloaded and installed this you can\\nopen up a ter teral or a command prompt\\nand inside of here you can just type AMA\\nto make sure this is working if for some\\nreason the command isn't recognized you\\ncan try running olama by just typing the\\nname of the application if you're on\\nsomething like Windows and double\\nclicking it to run now once olama is\\nworking you can pull an olama model that\\nyou want to run locally now to do that\\nyou type a llama pull and then you put\\nthe name of the model in this case I'll\\nuse a model like mistro but you can use\\nmodels like llama 3 and any open source\\nmodel really you want you can pull it\\nand run it locally assuming you have\\nsufficient hardware and all of the\\nhardware requirements are specified on\\nthe olama website where it lists all of\\nthe different open- Source models so I'm\\ngoing to pull the mistal model I already\\nhave this downloaded so now it's on my\\nmachine and then if I wanted to use this\\nmodel I could type llama run and then\\nmistl and then I can just start chatting\\nwith the model like I would in any other\\ncase now to leave I can type slash bu\\nand that's great and now we'll be able\\nto use AMA from our python code again I\\nhave an entire video in case you're\\nconfused or more help that I'll leave on\\nscreen okay so now that we've done that\\nwhat we're going to do is start setting\\nup a basic API to use our llm model now\\nagain you can use any llm that you want\\nhere the important thing is that we just\\nset up the API and we secure it properly\\nso for this video the IDE that I'm going\\nto be using is pycharm now this is one\\nof the best idees when it comes to\\nworking with python especially for\\nthings like apis and dealing with\\nFrameworks like Fast API Jango or flask\\nwhich we'll be using in this video now I\\nhave a long-term partnership with py and\\nif you guys want to get access to the\\nprofessional Edition with an extended\\nfree trial of up to 3 months you can do\\nthat by clicking the link in the\\ndescription py charm has two versions\\nThe Community Edition which is\\ncompletely free and the professional\\nEdition which you do need to pay for but\\nagain because you're a viewer of this\\nchannel I can give it to you 3 months\\nfor free so you can try it out and see\\nif you like it it has all kinds of great\\nfeatures and we can actually test our\\nAPI directly from the IDE which makes\\nour life a lot easier and you'll see\\nthat in this video again links in the\\ndescription so first things first we're\\njust going to set up the dependencies\\nfor our python project to build this API\\nso I'm going to make a new file I just\\nopened a folder here in this IDE so I\\nwent up here and I just opened a folder\\nI called it API for llm and then from\\nhere I'm going to go new file and then\\nI'm going to call this\\nrequirements.txt\\nokay from this requirements.txt file I'm\\njust going to specify the dependencies\\nthat I'll need for my python project and\\nobviously you need python installed in\\norder to follow along with the rest of\\nthese steps so I'm going to install fast\\nAPI UV corn olama Python\\nd.v and then requests okay now fast API\\nis what we'll use for building our API\\nit'll be very simple don't worry uicorn\\nis for running our fast API application\\nolama is for interfacing with olama to\\nuse a local llm and then python. EnV is\\nfor loading in an environment variable\\nfile and request is for Center request\\nwhen we later test our API now we've\\nspecified these in our requirements.txt\\nso the next step is to Simply install\\nthem so what we're going to do is just\\nopen up a new terminal instance I just\\nhave one here and make sure it's in the\\nsame folder where your requirements.txt\\nfile is if you're opening the terminal\\nfrom an IDE or an editor then it should\\njust already be in the correct directory\\nfrom here assuming we have python\\ninstalled we're going to type pip\\ninstall - R and then requirements.txt\\nnow this is just going to install all of\\nthe dependencies into our main python\\ninstallation if you want you can use a\\nvirtual environment but I'm not going to\\ncover that in the name of time if that\\ncommand doesn't work you can try pip 3\\ninstall - R requirements.txt and what\\nthis does is install all of the\\ndependencies from the requirements file\\nnow that we have all of that installed\\nwe can start writing some simple code\\nand get an API up and running so what\\nI'm going to do is make a new file and\\nit's going to be a python file and I'm\\njust going to call this main.py you can\\ncall it anything you want but I\\nrecommend naming it main just to stick\\nwith the um kind of conventions I'm\\nusing in this tutorial it'll be a little\\nbit easier to follow along with okay so\\nfrom here we're going to make a very\\nsimple API and to do that we're going to\\nsay from Fast API import and then fast\\nAPI and then we're going to import o\\nLama okay then we're going to make an\\napp so we're going to say app is equal\\nto fast API and we're going to Define an\\nendpoint an endpoint is just a URL on\\nthis server that we can access so for\\nnow we're going to say ATA dopost and\\nwe're going to do slash generate now you\\ncan call this anything that you want but\\nwhat this specifies is that you need to\\nsend a post request which is a type of\\nHTTP request to this URL and then the\\nfunction that we write which will be\\ncalled generate will run whenever we go\\nto this/ generate rout now inside of\\nhere we're going to take a prompt which\\nis a string and what this specifies is\\nthat you need to pass a query parameter\\nwhich looks like this prompt is equal to\\nand then whatever the prompt is whenever\\nyou want to call this rout so we know\\nwhat the prompt to the llm should be\\nfrom here we can generate a simple\\nresponse by saying response is equal to\\nol. chat from the chat we're going to\\nsay the model is equal to whatever model\\nwe pulled and we want to use in this\\ncase it's mistal then we're going to say\\nmessages is equal to we're going to\\nspecify a list and we're going to put a\\npython dictionary in the python\\ndictionary we need to specify the role\\nas user and then the content as The\\nPrompt okay so you can pass multiple\\nmessages here but if you just want to\\npass one then you just do it like this\\nand obviously you can modify the call to\\nthe llm but I'm just keeping it basic\\nfor right now after here we're going to\\nreturn the response and the way we'll do\\nthis is we'll return a python dictionary\\nthat has response and then the response\\nand this is going to be the message and\\nthe content just so that we strip out\\nthe information that we actually want so\\nthat's literally it we're saying okay we\\nwant to chat with AMA this works because\\nwe've downloaded on our machine we\\nspecify the model we want to use the\\nmessages that we want to pass you could\\nalso pass a system message if you wanted\\nto customize the prompt or something and\\nthen you can have a response we say\\nresponse message content okay now we\\nneed to run the API so to run the API we\\ncan open up our terminal and make sure\\nagain you're in the same directory where\\nyour python script is and you can type\\nuicorn and then main colon app-- reload\\nnow what these variables are right here\\nmain is the name of my file so main.py\\nand then app is the name of my fast API\\napplication so app so if you've changed\\neither of those you need to adjust these\\nright here and the D- reload will run\\nthis in development mode so it will\\nreload the server anytime any changes\\nare made so if I hit enter here you can\\nsee that now the API is running it tells\\nus what port it's running on so Port\\n8000 on Local Host and now we need to\\ntest it now there's various ways to test\\nthe API but if you're working inside of\\npy charm then you can open up this\\nendpoints tab you can find it by\\nclicking on these three dots and look\\nfor it here or you can see kind of a\\ncircle inside of a half circle from here\\nyou're going to specify the main. apppp\\npackage okay you're going to press the\\nslash generate so it can automatically\\nactually read um the endpoint and then\\nwhat you can do is see the kind of\\nsample request right here so you can see\\nit says post local hostport 8000 SL\\ngenerate and then you'll need to add\\nthis query parameter question mark\\nprompt equal to and then whatever you\\nwant to prompt the llm with once you do\\nthat you can hit submit request you can\\nwait for this to run it will take a\\nsecond and then you can see the response\\nright here so let me just make this a\\nlittle bit bigger so that you guys can\\nread this and you can see that we get\\nthe response from the llm it says hello\\nworld it's a classic example blah blah\\nblah blah blah and you can use it as you\\nsee fit now other than that the easiest\\nway to test this if you're not using\\nsomething like py charm is to download a\\ntool called postmen so I'm just going to\\nopen it up right here this is a free\\ntool that you can download on your\\ncomputer whether it's window Mac Linux\\never that allows you to test your apis\\nreally easily so if you don't have\\nfamiliarity testing them simply download\\nand install this tool and then open it\\nup and I'll show you how to use it so\\nassuming our API is still running in our\\nterminal we can open up the postman\\napplication from here we can press this\\nlittle plus button you might need to\\nmake an account or it might ask you for\\nsome stuff you don't need to but just\\nget into this workspace press on plus\\nand then what you're going to do is\\nchange get here to say post because\\nwe're sending a post request from here\\nyou're going to type in the URL so HTTP\\ncolon colon SL localhost SL generate\\nokay and we need to make sure this is\\nPort 8000 so sorry Local Host Port 8000\\nSL generate question mark prompt is\\nequal to whatever you want to prompt so\\nsomething like hello world and then what\\nwe can do is we can press on send\\nPostman will automatically send the\\nrequest for us again you just need to\\nmake sure you have a URL that looks like\\nthis when we do this you can then see\\nthat we get the response back from the\\nllm\\nokay so now we've written a basic llm\\nbut we want to make this secure we want\\nto add some kind of authorization or at\\nleast some logic so that not anyone can\\njust use this so what I'm going to do\\nnow is adjust this code so that only\\nsomeone that has access to our function\\nor to our API will be able to use the\\nllm because after all that's the entire\\npoint of this we don't want to let just\\na random person use the llm we want to\\nmaybe control their access okay so now\\nlet's add our own version of an API key\\nnow this is a key that someone will need\\nto send to our backend server if they\\nwant to utilize the API now what we can\\ndo with this key is we can just give it\\nto certain users or we can generate it\\nhowever we want in any way that we see\\nfit and we can have some kind of value\\nattached to this key to limit the number\\nof requests that someone could send so\\nfor example any API key that I give to\\nsomeone maybe it only has a limit of 10\\nso after you've used the key 10 times\\nmaybe you need to pay more money or you\\nneed to wait a day or something\\nsomething like that before you could use\\nthe key again I'm going to set up some\\nbasic logic but it will give you a sense\\nof how to do this so what I'm going to\\ndo is I'm going to import a few things\\nso from Fast API I'm going to import\\ndepends HTTP exception and header okay\\nI'm then going to import OS and I'm\\ngoing to say from.\\nEnV import load. EnV now I'm going to\\ncall the load. EnV function and what\\nthis is going to do is load a value from\\nour environment variable iable file\\nwhich I'm going to create right now so\\ninside of our directory we're going to\\nmake a new file and we're going to call\\nthis EnV okay inside of here we're going\\nto say API key is equal to and then we\\ncan set this to some secret key that we\\nwould only give people that we want to\\nhave access to our API in order to use\\nthe llm now you could have as many API\\nKeys as you want and typically you would\\ngenerate these for users so when people\\nhave an account they would sign in and\\nthen you could issue them an API key and\\nthey need to use that API key to access\\nthe API that's a more advanced topic and\\nI'll leave a video on screen that shows\\nexactly how to do that but you'll get\\nthe idea with what I show you right here\\nso in this EnV file we've specified a\\nvariable called secret key now what\\nwe're going to do is we're going to load\\nin that value so we're going to say API\\nKeys is equal to a list or sorry is\\nequal to a set and inside of here we're\\ngoing to say os. getv and then the API\\nkey now this works because we've l\\nloaded in the environment variable file\\nand now we can get that environment\\nvariable and we put that in our list of\\nvalid API keys and if we wanted to we\\ncould associate this with a value like\\n10 and then we could subtract from this\\nvalue anytime someone uses the API and\\ntreat this as like their credits so I\\ncan say you know this key has five\\ncredits remaining so we could change\\nthis to API key credits or something and\\nyou know what let's go ahead and do that\\nokay now that we've done that we're\\ngoing to write a simple function that\\nwill verify that a user has an API key\\nbefore we allow them to continue so I'm\\ngoing to say Define verify uncore aior\\nkey and we're going to specify xcore\\naior key we're going to say this is a\\nstring and this is equal to header and\\nthen none okay now what this is going to\\ndo is it's going to look in the headers\\nof our request which I'll show you in\\none second for a variable called X API\\nkey if finds it it's going to associate\\nit with this parameter right here and\\nthen allow us to read it in and use it\\ninside of this um what do you call it\\nfunction so what we're actually going to\\ndo here is we're going to say credits is\\nequal to API key credits. getet xapi key\\nnow we're going to specify the default\\nvalue is zero and what this means is\\nthat we're going to attempt to get\\nwhatever the number of credits are\\nassociated with our API key but if that\\nAPI key doesn't exist then we're just\\ngoing to get zero then we're simply\\ngoing to check if the credits are equal\\nto zero so we're going to say if the\\ncredits are less than or equal to zero\\nthen what we're going to do is we're\\ngoing to return or sorry we're going to\\nraise an HTTP exception and we're going\\nto say status 401 and then we're going\\nto say invalid API key or no credits\\nokay just telling them hey you don't\\nhave any credits left or your API key is\\nexpired or actually from here we can\\njust return the X API key so if they do\\nhave a valid key we can return it so we\\ncan use it later on and then inside of\\nhere we're going to we're going to make\\nsure that they have an API key and that\\nit's valid before we allow them to use\\nthis function so to do that inside of\\nthe parameters we're going to add X _\\nAPI key we're going to say this is type\\nstring and this is equal to depends on\\nverify API key now just let me use this\\nvariable and then it will light up so\\nyou can read it easier but what this\\nmeans is that we're depending on this\\nfunction call and then if this function\\ncall raises an error we just won't do\\nanything inside of here whereas if it\\nReturns the API key then we're good to\\ngo and we'll enter this function so\\ninside of here the first thing we're\\ngoing to do is just subtract our credits\\nso we're going to say API key credits at\\nX API key and then we're just going to\\nsay minus equals 1 so we're just going\\nto subtract one from that to specify hey\\nwe're removing one of your credits\\nbecause now you've successfully used\\nthis function and that's it that's\\nliterally all that we need and now we've\\nadded kind of control to this API where\\nyou can only use this if you pass us an\\nAPI key now again we've just hardcoded\\nan API key here from our environment\\nvariable file F but you could\\ndynamically generate these however you\\nwanted to and you can handle them for\\nvarious users so I'm showing you the\\nlogic but obviously you would need to go\\na step further with this to have correct\\nauthentication and authorization for a\\nlarger Scale app so now that we've done\\nthis we want to test the authorized API\\nso we should be able to rerun this so\\nhere it looks like it was rerunning but\\nI'm just going to shut it down and\\nrestart it to make sure it loads in the\\nenvironment variable so same thing\\nuvicorn main app reload so now to test\\nthe API again there's multiple ways to\\ndo that but I'm just going to use\\nPostman so that it's universally\\navailable for all of you and I'm going\\nto keep the same request that I had last\\ntime except this time I'm going to go\\ninto headers now inside of headers here\\nI'm just going to add the header for my\\nAPI key so I'm going to add the header\\nof xcore API uncore key like that I\\nbelieve that's what we called it and\\nthen for the value I'm going to say\\nsecret key okay all right so a silly\\nmistake here but actually we want to use\\nuh High when we specify this in the\\nheader not underscores so just x-\\napi-key and then put the value secret\\nkey you don't need any quotes and now\\nwhen we send this you should see that it\\nworks it just takes a second and then we\\nget the response back and if we try to\\ndo this now five times let's just keep\\ndoing it so that our credits go lower\\nand lower you should see eventually when\\nwe get to the fifth request that it\\ndoesn't work and it tells us that we run\\nout of tokens and you can see that's\\nwhat we get it says invalid API key or\\nno credits so there you go we've now\\nbuilt an authenticated API that allows\\nus to utilize our llm with this API key\\nnow again you want to manage these Keys\\ncorrectly there's a lot of other logic\\nand things you can do and I will leave a\\nvideo on screen that will help you do\\nthat now beyond that I'm quickly just\\ngoing to show you a python script that\\nyou can use if you wanted to test the\\nAPI from python code so I'm just going\\nto make a script called test API now\\nwhat I'm going to do is just copy in\\nsome code so here is the code you can\\nsee that I import requests I import the\\ncode to load my environment variable\\nfile I specify the URL that I want to\\nsend my request to and the prompt that I\\nwant to pass to the llm I then specify\\nmy headers so I have my API key which is\\nequal to what I'm loading from the\\nenvironment variable file content type\\napplication Json send a post request\\nwith my URL and my headers and then I\\nprint out the response. Json now I just\\nrestarted my server I took a quick cut\\nso when I restarted the server it will\\nreset the amount of credits that I have\\nand now if I run this file just takes a\\nsecond and we should see that we get our\\nresponse and we do you can see that I\\nget the response back from the llm okay\\nso that's pretty much all I wanted to\\nshow you in this video I know there was\\na lot but I wanted to explain how you\\nset up a simple API with this API token\\nwhich you can then use to control access\\nto the llm the llm call here can be\\nanything that you want but it's\\nimportant that you do gate it and you do\\ncontrol it from a backend server so that\\nfront-end users can't directly access\\nyour tokens now you may be asking well\\nwhat about this token right here the\\nwhole point of this token is that you\\ncontrol this token rather than open AI\\nor deep seek or something like that so\\nnow if you want to invalidate the token\\nif you want to have a certain number of\\ncredits if you only want to give it to\\npremium users or paid users you can do\\nthat right you can control how you want\\nto issue this token but if you just use\\nan openai token now anyone that has\\naccess to that can go and utilize open\\naai and cost you a ton of money so\\nthat's why you want to set up this own\\nkind of custom back end where you have\\nfull control as the developer and the\\napp owner I will leave this code in the\\ndescription in case you want to check it\\nout it'll be available from GitHub if\\nyou enjoyed the video make sure to leave\\na like subscribe to the channel and I\\nwill see you in the next one\\n[Music]\",\n",
       " 'blog': AIMessage(content='### Building a Secure API for Your AI Model: A Step-by-Step Guide\\n\\nIn the ever-evolving world of AI, models like ChatGPT and DeepSeek are becoming increasingly popular. However, their usage isn’t without challenges, particularly in terms of security and cost control. This blog post will guide you through creating a simple, secure API for your AI model, giving you the power to control access and keep costs under check.\\n\\n#### Why Control Access to Your AI Model?\\n\\nImagine you\\'re deploying an AI model in a production environment. Directly using the API key from your frontend (e.g., a website or mobile application) is like leaving your door unlocked. Anyone with access to your frontend code can read and use your API key, potentially racking up huge cloud computing costs or exhausting your local compute resources.\\n\\n#### Setting Up Your Environment\\n\\nTo start, we’ll set up a Python API using FastAPI, a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints. For our model, we’ll use AMALocal, a powerful tool to run large language models directly on your local machine.\\n\\n1. **Install AMALocal**  \\n   Follow the instructions on the AMALocal website to download and install the software. Once installed, you can load a model like Mistro or LLaMA-3 by using the command `amalocal pull mistral`.\\n\\n2. **Setting Up the Project**  \\n   Open PyCharm, create a new project, and set up a `requirements.txt` file to ensure you have everything you need:\\n   ```python\\n   fastapi\\n   uvicorn\\n   amalocal\\n   python-dotenv\\n   requests\\n   ```\\n\\n#### Crafting the API\\n\\nLet’s dive into the code. The key is to control how and when the AI model is accessed. Here’s a simple FastAPI setup:\\n\\n```python\\nfrom fastapi import FastAPI, HTTPException, Header\\nimport os\\nfrom dotenv import load_dotenv\\nfrom amalocal import amalocal\\n\\nload_dotenv()\\nAMALocalModel = amalocal.load_model(name=\"mistral\")\\n\\napp = FastAPI()\\n\\nAPI_KEY = os.getenv(\"API_KEY\")\\nAPI_KEY_CREDITS = {API_KEY: 10}  # Example credits system\\n\\ndef verify_api_key(x_api_key: str = Header(None)):\\n    if x_api_key not in API_KEY_CREDITS or API_KEY_CREDITS[x_api_key] <= 0:\\n        raise HTTPException(status_code=401, detail=\"Invalid API key or no credits\")\\n    API_KEY_CREDITS[x_api_key] -= 1\\n    return x_api_key\\n\\n@app.post(\"/generate\")\\ndef generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\\n    response = AMALocalModel.chat([{\"role\": \"user\", \"content\": prompt}])\\n    return {\"response\": response[\"message\"][\"content\"]}\\n\\nif __name__ == \"__main__\":\\n    import uvicorn\\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000, reload=True)\\n```\\n\\nThis code does a few things:\\n- Loads your AMALocal model.\\n- Verifies the API key sent in the header.\\n- Decreases the API key’s credit by one on each call.\\n- Responds with the model’s generated text.\\n\\n#### Testing Your API\\n\\nTo test, you can use Postman or run a Python script that sends requests to your API:\\n\\n```python\\nimport requests\\nimport dotenv\\nimport os\\n\\ndotenv.load_dotenv()\\nAPI_URL = \"http://localhost:8000/generate\"\\nAPI_KEY = os.getenv(\"API_KEY\")\\n\\nresponse = requests.post(API_URL, headers={\"X-API-KEY\": API_KEY}, json={\"prompt\": \"Tell me a joke.\"})\\nprint(response.json())\\n```\\n\\n#### Final Thoughts\\n\\nCreating a secure API for your AI model is crucial for avoiding unauthorized usage and keeping costs in check. The example here is a starting point. In a real-world scenario, you’d want to implement more robust authentication mechanisms and possibly integrate a database to manage user credits dynamically.\\n\\nFeel free to download the full project from GitHub and play around with it. Remember, the key to mastering these tools is practice and experimentation. Happy coding!\\n\\n---\\n\\nThis blog post takes you through the process of creating a secure API for your AI model, ensuring that your resources remain under control and that you maintain full ownership over who gets access to your powerful AI capabilities.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 936, 'prompt_tokens': 5692, 'total_tokens': 6628, 'completion_time': 4.68, 'prompt_time': 0.268972671, 'queue_time': 0.47401496099999996, 'total_time': 4.948972671}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-8fcc87ab-5601-4f04-b9df-c4fe8bd3f0de-0', usage_metadata={'input_tokens': 5692, 'output_tokens': 936, 'total_tokens': 6628}),\n",
       " 'review': AIMessage(content='### Review of \"Building a Secure API for Your AI Model: A Step-by-Step Guide\"\\n\\nThe blog post \"Building a Secure API for Your AI Model: A Step-by-Step Guide\" offers a comprehensive and practical guide on how to create a secure API for AI models, focusing particularly on cost control and access management. The author\\'s approach is both detailed and accessible, making it a valuable resource for developers and data scientists looking to deploy AI models in a secure and controlled manner.\\n\\n#### Strengths of the Blog Post\\n\\n1. **Clarity and Practicality**: The blog post is exceptionally clear in its explanations and provides practical, actionable steps for setting up a secure API. It begins with a relatable problem statement about the risks of exposing your API key directly and then logically progresses into the technical details.\\n\\n2. **Code Examples**: The inclusion of code snippets, such as the FastAPI setup and the client-side testing script, is a significant strength. It not only helps in understanding the concepts but also allows readers to follow along and implement the solution in their own environment.\\n\\n3. **Security Focus**: The emphasis on API key management and the example of a simple credit system for API calls is particularly insightful. It introduces beginners to the importance of securing AI models and managing resource usage effectively.\\n\\n4. **Further Improvement Suggestion**: The author mentions adding more sophisticated authentication methods and integrating a database for dynamic user credit management, which gives readers a roadmap for further development and improvement.\\n\\n5. **Interactive Element**: By providing a GitHub link for downloading the full project, the author encourages hands-on learning and experimentation, which is crucial for mastering such technologies.\\n\\n#### Areas for Improvement\\n\\n1. **Depth of Explanation**: While the blog post is thorough, there could be more explanation around the chosen technologies like AMALocal and FastAPI. A brief introduction or link to more detailed resources could enrich the understanding for those not familiar with these tools.\\n\\n2. **Security Best Practices**: While the blog post does cover basic security measures, a deeper dive into best practices for securing APIs, such as rate limiting, IP whitelisting, or even more advanced authentication methods, could provide a more comprehensive security setup.\\n\\n3. **Scalability Considerations**: The discussion around handling multiple users or scaling the API could be expanded. Although the blog post introduces the idea of a credit system, it does not delve into how this might be managed at scale, which would be useful for readers planning to deploy this in a production environment.\\n\\n#### Conclusion\\n\\nOverall, \"Building a Secure API for Your AI Model: A Step-by-Step Guide\" is a well-written, practical guide that addresses a critical issue in the deployment of AI models. The detailed steps and clear code examples make it an excellent resource for anyone looking to secure and control their AI deployment. The inclusion of a GitHub project for download is a great addition, encouraging readers to apply what they\\'ve learned immediately. While there is room for deeper dives into certain aspects, the blog post successfully achieves its primary goal of educating its audience on securing their AI models through a secure API setup.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 625, 'prompt_tokens': 1279, 'total_tokens': 1904, 'completion_time': 3.125, 'prompt_time': 0.061539562, 'queue_time': 0.23786200000000002, 'total_time': 3.186539562}, 'model_name': 'qwen-2.5-32b', 'system_fingerprint': 'fp_35f92f8282', 'finish_reason': 'stop', 'logprobs': None}, id='run-fd382067-88ef-4cfc-ba31-ec12f7202231-0', usage_metadata={'input_tokens': 1279, 'output_tokens': 625, 'total_tokens': 1904}),\n",
       " 'human_feedback': 'Blog should be creative'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Securing Access to AI Models with a Simple Python API: A Creative Guide\n",
       "\n",
       "AI models are incredibly powerful but, just like any powerful tool, they come with their own set of responsibilities—chief among them, ensuring that they're used securely and efficiently. In this blog post, we'll dive into how to create a simple Python API to control access to your AI model, whether it's a local model or one hosted on a cloud service. Let’s embark on this adventure, making sure we wield our AI models with care and precision.\n",
       "\n",
       "#### Why Secure Your AI Model?\n",
       "\n",
       "Before diving into the code, let’s understand why securing your AI model is essential. Imagine your AI model as a magical portal to the world of intelligent responses. Without proper control, anyone with access can tap into this portal, potentially draining your resources or even compromising its integrity. By controlling, we mean ensuring only authorized users have access, and each use is tracked, much like a magical token that can only be used a certain number of times.\n",
       "\n",
       "#### Building the Foundation\n",
       "\n",
       "To get started, you’ll need to set up your environment. For this guide, we’ll use `FastAPI`, a modern, fast (high-performance) web framework for building APIs with Python 3.7+ based on standard Python type hints. Our AI model will be a local model, and we’ll use `olama`, a tool that lets you run models on your own machine. This is a free, open-source treasure that you can download and install based on your hardware capabilities.\n",
       "\n",
       "#### Crafting the API\n",
       "\n",
       "1. **Setting Up Your Environment:**\n",
       "\n",
       "   First, create a `requirements.txt` file with all the necessary dependencies. Our dependencies include `FastAPI`, `uvicorn` for running the server, and `olama` for interfacing with the local AI model. Install these by running `pip install -r requirements.txt` in your terminal.\n",
       "\n",
       "2. **Creating the API Structure:**\n",
       "\n",
       "   In your Python file, which we’ll name `main.py`, start by importing the necessary modules and setting up the FastAPI application.\n",
       "\n",
       "   ```python\n",
       "   from fastapi import FastAPI, HTTPException\n",
       "   import olama\n",
       "   import os\n",
       "   from dotenv import load_dotenv\n",
       "\n",
       "   # Load environment variables\n",
       "   load_dotenv()\n",
       "   ```\n",
       "\n",
       "3. **Securing the API:**\n",
       "\n",
       "   The heart of this project is ensuring that only those with the right access (think of it as a magical key) can use the model. For this, we’ll simulate an API key system.\n",
       "\n",
       "   ```python\n",
       "   # Define the API key\n",
       "   API_KEYS = set(os.getenv('API_KEYS').split(','))\n",
       "\n",
       "   def verify_api_key(x_api_key: str = Header(...)):\n",
       "       \"\"\"\n",
       "       Verify the API key.\n",
       "       If the key is valid, return it. If not, raise an exception.\n",
       "       \"\"\"\n",
       "       if x_api_key not in API_KEYS:\n",
       "           raise HTTPException(status_code=401, detail=\"Invalid API Key\")\n",
       "       return x_api_key\n",
       "   ```\n",
       "\n",
       "   In your `main.py`, specify an endpoint, `/generate`, that requires a POST request and an API key.\n",
       "\n",
       "   ```python\n",
       "   @app.post(\"/generate\")\n",
       "   def generate(prompt: str, x_api_key: str = Depends(verify_api_key)):\n",
       "       # Check credits and generate response\n",
       "       return olama.chat(model=\"mistol\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
       "   ```\n",
       "\n",
       "4. **Testing the API:**\n",
       "\n",
       "   Once your API is up and running, test it using Postman (a magical wand for HTTP requests). Yes, it’s that cool). You’ll need to specify the URL (`http://localhost:8000/generate`), send a POST request, and include the API key as a header. If all goes well, you’ll see the AI model’s response, and if your API key runs out, it’ll politely refuse to play.\n",
       "\n",
       "#### Finishing Touches\n",
       "\n",
       "Adding an authentication layer to your API is like adding a guardian dragon to protect your digital treasure. Not only does it keep the intruders out, but it also allows you to control how your AI is used, ensuring it’s a valuable and secure resource.\n",
       "\n",
       "#### Wrapping Up\n",
       "\n",
       "By securing your AI model with a simple Python API, you’re not just protecting your resources; you’re taking a step towards building a more secure and controlled environment for your AI applications. Whether you’re a beginner or a seasoned developer, the principles here can be adapted and expanded upon to suit more complex needs.\n",
       "\n",
       "If you enjoyed this journey into securing AI models with Python, be sure to follow more tutorials on this blog for more magical coding adventures!\n",
       "\n",
       "--- \n",
       "\n",
       "This creative twist on your transcript adds a bit of fantasy and narrative to the technical process, making it more engaging and easier to understand."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Run the workflow\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "final_state = workflow.invoke({\"video_url\": \"https://www.youtube.com/watch?v=cy6EAp4iNN4\"}, thread)\n",
    "\n",
    "# Retrieve the final blog content\n",
    "#final_blog = final_state.get(\"blog\")\n",
    "\n",
    "# Display the final blog content if it exists\n",
    "if final_blog:\n",
    "    display(Markdown(final_state['blog'].content))\n",
    "else:\n",
    "    print(\"No blog content generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_state = workflow.invoke({\"video_url\": \"https://www.youtube.com/watch?v=cy6EAp4iNN4\"}, thread)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"### Crafting Secure Access to AI Models with Python APIs\\n\\nIn the realm of artificial intelligence, where large language models (LLMs) like ChatGPT and DeepSeek are becoming increasingly powerful, it's crucial to ensure that these tools are used both effectively and securely. Today, we delve into how to create a simple yet robust Python API that controls access to an LLM, ensuring that it's not only functional but also secure from unauthorized use.\\n\\n#### Why Secure API Control is Essential\\n\\nImagine you're leveraging an LLM to power a feature in your application. Whether it's running locally on your machine or through a cloud provider like OpenAI, the API key for accessing these services can be a juicy target for misuse, especially if it's exposed in your frontend code. This isn't just about safeguarding against external threats; it's also about ensuring that your application remains financially and resource-efficient.\\n\\n#### Setting Up the Environment\\n\\nBefore diving into the coding, let's get our environment ready. For this demo, we'll be using a local installation of an LLM with a tool called Lama (yes, another AI-related pun). We love them here!). With a simple installation, you can launch any open-source LLM on your machine, provided you have the necessary hardware. No need to worry about the cloud costs here, but the principles remain the same.\\n\\n#### Building the API with FastAPI\\n\\nFastAPI is our framework of choice for this project due to its simplicity and the powerful features it offers out of the box. Our goal is straightforward: create an API that accepts requests, verifies the user based on an API key, and then, if everything checks out, interacts with the LLM.\\n\\n**Step 1: Environment Setup**\\n\\nFirst, let's set up our Python environment. We'll need FastAPI, Uvicorn for running our API, and requests to test our API. This is where the `requirements.txt` file comes in handy, detailing all the dependencies we need to install with a simple `pip install -r requirements.txt`.\\n\\n**Step 2: Creating the API**\\n\\nIn our main Python file (`main.py`), we'll define our FastAPI app. The main logic here involves a simple endpoint (`/generate`) that requires a POST request with a prompt to the LLM. But how do we ensure only authorized access?\\n\\n**Security: Authorization with API Keys**\\n\\nHere's where the fun part begins. We're going to introduce API key authentication. This key, loaded from an environment variable file (`env`), acts as the entry pass to our API.\\n\\n#### Testing the API\\n\\nTesting is as simple as it gets. With Postman, a universal API testing tool, we can simulate API requests. We test our setup with a valid API key, verifying that it works as expected. We even simulate a scenario that depletes the API key's credits, showcasing a real-life use case.\\n\\n#### Conclusion\\n\\nBy the end of this tutorial, not only will you have a working Python API to control access to your LLM, but you'll also have a deeper understanding of API security, from the fundamentals to practical implementation. This setup not only ensures that your application remains secure from unauthorized use but also gives you control over how your LLM is used, whether it's through user limits, cost management, or other business logic.\\n\\nWith that, we hope you've gained a practical and secure foundation for integrating LLMs into your applications. For those eager to explore more, don't forget to check out additional resources for more advanced API management and authentication methods. Happy coding!\\n\\n[Music]\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_state['blog'].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
